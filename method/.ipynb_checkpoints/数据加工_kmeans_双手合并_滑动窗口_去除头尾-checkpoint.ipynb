{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1082ba75-937f-42f7-a67f-1eb343329e7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import load\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a351e3ad-10b9-4294-8815-45653c7514c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#参数列表\n",
    "kmeans_loaded = load('../model/kmeans/kmeans_40_双手合并.joblib') #分类文件夹根目录\n",
    "all_path = '../SLR_dataset/seq_txt/'\n",
    "save_path = '../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/'\n",
    "all_folder = [i for i in os.listdir(all_path)]\n",
    "quchong = True #去重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "fb3ecf74-a236-48f7-a8cf-fdcf21379941",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目录已创建：../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/\n"
     ]
    }
   ],
   "source": [
    "# 检查路径是否存在，不存在则创建（包括父目录）\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "    print(f\"目录已创建：{save_path}\")\n",
    "else:\n",
    "    print(f\"目录已存在：{save_path}\")\n",
    "def left_right_hand(data):\n",
    "    left = data[0:63]\n",
    "    right = data [63:]\n",
    "    return left,right\n",
    "def list_proLR(data):\n",
    "    #该函数用于将读取到的数据变成左右手格式\n",
    "    result = []\n",
    "    for i in data:\n",
    "        left,right = left_right_hand(i)\n",
    "        result+=[left,right]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "abc982b6-e49c-470c-a847-7d3801d907d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_consecutive_duplicates(sequence):\n",
    "    if not sequence:  # 空列表直接返回\n",
    "        return []\n",
    "    \n",
    "    result = [sequence[0]]  # 初始化结果列表，包含第一个元素\n",
    "    for num in sequence[1:]:  # 从第二个元素开始遍历\n",
    "        if num != result[-1]:  # 当前元素与结果中最后一个元素不同\n",
    "            result.append(num)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "327a21a0-2fe4-4b98-bc3f-75230b32c200",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_with_outlier(X, threshold=0.1):\n",
    "    # distances = kmeans_loaded.transform(X)  # 计算到所有中心的距离\n",
    "    # min_distances = np.min(distances, axis=1)\n",
    "    \n",
    "    # 分类结果（离群点设为0）\n",
    "    labels = kmeans_loaded.predict(X)\n",
    "    # labels[min_distances > threshold] = 999  # 阈值需根据数据分布调整\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6cd8037c-062b-4126-bd4a-e56076fef794",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def smart_clean(seq, max_repeats=1):\n",
    "    cleaned = []\n",
    "    i = 0\n",
    "    n = len(seq)\n",
    "    \n",
    "    while i < n:\n",
    "        best_pattern = None\n",
    "        best_length = 0\n",
    "        \n",
    "        # 检测从当前位置开始的最长重复模式（包括单个元素的重复）\n",
    "        for pattern_len in range(1, 5):  # 检测1到4长度的模式\n",
    "            if i + pattern_len > n:\n",
    "                continue\n",
    "                \n",
    "            pattern = seq[i:i+pattern_len]\n",
    "            repeats = 1\n",
    "            \n",
    "            # 计算重复次数\n",
    "            while i + (repeats + 1) * pattern_len <= n:\n",
    "                next_segment = seq[i + repeats * pattern_len : i + (repeats + 1) * pattern_len]\n",
    "                if next_segment == pattern:\n",
    "                    repeats += 1\n",
    "                else:\n",
    "                    break\n",
    "            \n",
    "            # 如果重复次数超过阈值，且比之前找到的模式更长，则更新最佳模式\n",
    "            if repeats > max_repeats and pattern_len * repeats > best_length:\n",
    "                best_pattern = pattern\n",
    "                best_length = pattern_len * repeats\n",
    "        \n",
    "        if best_pattern:\n",
    "            # 保留1次模式（如果是单个元素重复，只保留1次）\n",
    "            cleaned.extend(best_pattern)\n",
    "            i += best_length\n",
    "        else:\n",
    "            cleaned.append(seq[i])\n",
    "            i += 1\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "2f74fa6d-45c1-4942-84fa-6dbabed3ffdf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000000.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000001.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000002.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000003.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000004.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000005.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000006.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000007.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000008.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000009.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000010.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000011.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000012.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000013.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000014.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000015.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000016.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000017.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000018.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000019.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000020.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000021.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000022.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000023.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000024.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000025.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000026.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000027.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000028.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000029.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000030.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000031.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000032.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000033.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000034.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000035.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000036.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000037.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000038.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000039.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000040.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000041.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000042.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000043.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000044.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000045.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000046.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000047.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000048.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000049.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000050.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000051.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000052.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000053.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000054.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000055.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000056.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000057.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000058.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000059.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000060.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000061.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000062.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000063.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000064.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000065.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000066.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000067.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000068.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000069.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000070.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000071.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000072.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000073.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000074.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000075.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000076.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000077.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000078.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000079.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000080.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000081.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000082.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000083.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000084.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000085.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000086.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000087.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000088.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000089.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000090.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000091.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000092.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000093.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000094.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000095.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000096.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000097.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000098.npy\n",
      "../SLR_dataset/kmeans_40_seq_双手合并_数据去重_三次滑动/000099.npy\n",
      "133\n"
     ]
    }
   ],
   "source": [
    "maxlen = 0\n",
    "for i in all_folder:\n",
    "    all_new_labels = []\n",
    "    folder = os.path.join(all_path,i)\n",
    "    file_name = [file for file in os.listdir(folder) if file.endswith('.avi')]\n",
    "    for j in file_name:\n",
    "        file_path = os.path.join(folder,j)\n",
    "        datalist = []\n",
    "        with open(file_path, 'r') as file:  # 以读取模式打开文件\n",
    "            for line in file:       # 逐行读取文件内容\n",
    "                # 按空格分割每一行，并将每个部分转换为浮点型\n",
    "                float_values = [float(x) for x in line.split()]\n",
    "                datalist.append(float_values)  # 将转换后的浮点型列表添加到 datalist\n",
    "        new_labels = predict_with_outlier(datalist)\n",
    "        # print(len(new_labels)) #212\n",
    "        # new_labels = [i for i in new_labels]\n",
    "        # # print(new_labels)\n",
    "        all_new_labels.append(new_labels)\n",
    "    all_new_labels = [[x for x in j if x != 999] for j in all_new_labels]\n",
    "    for cishu in range(3):\n",
    "        all_new_labels = [smart_clean(seq, max_repeats=1) for seq in all_new_labels]\n",
    "    # print(len(all_new_labels))\n",
    "    for seq in all_new_labels:\n",
    "        if len(seq) > maxlen:\n",
    "            maxlen = len(seq)\n",
    "    save_file_name = os.path.join(save_path,i)+'.npy'\n",
    "    save_data = np.array(all_new_labels,dtype=object)\n",
    "    # print(save_data)\n",
    "    np.save(save_file_name,save_data)\n",
    "    print(save_file_name)\n",
    "        # all_new_labels.append(remove_consecutive_duplicates(new_labels))\n",
    "print(maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9754c5d-7f11-4c73-a8bb-0bfdb108c4fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c31f11c4-a332-4ce3-bbb3-5fb1f4dfae49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for i in all_new_labels:\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "670aff84-fa1a-475e-b671-72b65cfb6626",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_new_labels = [[x for x in j if x != 999] for j in all_new_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9ecf394f-c270-4a6d-9d02-5ce876029072",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_new_labels = [smart_clean(seq, max_repeats=1) for seq in all_new_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc79e827-65ab-45f1-91a2-898bbe6da23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0cd694a9-733f-4927-9aad-8b1d79fdd7e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4b487b4f-ec31-490a-bdcc-f9ac5168b1f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cleandata = []\n",
    "for i in all_new_labels:\n",
    "    cleandata.append(smart_clean(smart_clean(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e8dc2a18-8256-4161-8417-37e1a2c806d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 33, 26, 15, 6, 2, 15, 6, 20, 38, 1, 4, 1, 4, 1, 4, 38, 4, 38, 4, 38, 4, 38, 4, 38, 4, 38, 20, 13, 38, 1, 4, 1, 4, 38, 4, 38, 20, 38, 4, 38, 1, 6, 1, 4, 1, 4, 10, 37, 27, 1, 20, 1, 38, 4, 38, 4, 38, 37, 38, 37, 10, 37, 10, 37, 10, 37, 6, 10, 37, 10, 37, 6]\n",
      "[6, 33, 15, 33, 29, 33, 6, 38, 1, 4, 1, 38, 20, 38, 6, 20, 6, 38, 6, 10, 37, 38, 4, 38, 4, 38, 6, 38, 1, 6, 1, 10, 27, 10, 37, 1, 4, 1, 4, 1, 4, 10, 37, 1, 4, 1, 37, 1, 17, 10, 37, 10, 37, 10, 37, 10, 37, 10, 1, 37, 10, 20, 1, 10, 20, 1, 10, 1, 37, 10, 37, 1, 6, 1, 6]\n",
      "[6, 33, 6, 2, 30, 25, 32, 25, 32, 8, 32, 10, 37, 1, 39, 28, 1, 24, 17, 36, 17, 0, 17, 0, 17, 0, 17, 0, 17, 2, 32, 2, 9, 2, 35, 5]\n",
      "[6, 33, 6, 1, 6, 10, 37, 10, 37, 10, 6, 10, 0, 17, 1, 17, 1, 39, 28, 24, 1, 24, 17, 0, 17, 36, 17, 0, 17, 0, 17, 2, 32, 2, 9, 2, 35, 5]\n",
      "[6, 33, 15, 2, 3, 2, 30, 25, 32, 8, 32, 10, 18, 10, 37, 1, 4, 39, 28, 24, 1, 24, 17, 0, 2, 8, 2, 35, 5]\n"
     ]
    }
   ],
   "source": [
    "for i in all_new_labels[:5]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6b12a04d-9a58-406a-9be7-f67f8fdcb167",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 33, 26, 15, 6, 2, 15, 6, 20, 38, 1, 4, 38, 4, 38, 20, 13, 38, 1, 4, 1, 4, 38, 4, 38, 20, 38, 4, 38, 1, 6, 1, 4, 1, 4, 10, 37, 27, 1, 20, 1, 38, 4, 38, 4, 38, 37, 38, 37, 10, 37, 6, 10, 37, 10, 37, 6]\n",
      "[6, 33, 15, 33, 29, 33, 6, 38, 1, 4, 1, 38, 20, 38, 6, 20, 6, 38, 6, 10, 37, 38, 4, 38, 4, 38, 6, 38, 1, 6, 1, 10, 27, 10, 37, 1, 4, 10, 37, 1, 4, 1, 37, 1, 17, 10, 37, 10, 1, 37, 10, 20, 1, 10, 20, 1, 10, 1, 37, 10, 37, 1, 6, 1, 6]\n",
      "[6, 33, 6, 2, 30, 25, 32, 25, 32, 8, 32, 10, 37, 1, 39, 28, 1, 24, 17, 36, 17, 0, 17, 2, 32, 2, 9, 2, 35, 5]\n",
      "[6, 33, 6, 1, 6, 10, 37, 10, 37, 10, 6, 10, 0, 17, 1, 17, 1, 39, 28, 24, 1, 24, 17, 0, 17, 36, 17, 0, 17, 0, 17, 2, 32, 2, 9, 2, 35, 5]\n",
      "[6, 33, 15, 2, 3, 2, 30, 25, 32, 8, 32, 10, 18, 10, 37, 1, 4, 39, 28, 24, 1, 24, 17, 0, 2, 8, 2, 35, 5]\n"
     ]
    }
   ],
   "source": [
    "for i in cleandata[:5]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27588890-289f-4be6-ab71-bfe3938b6c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in all_folder:\n",
    "    all_new_labels = []\n",
    "    folder = os.path.join(all_path,i)\n",
    "    file_name = [file for file in os.listdir(folder) if file.endswith('.avi')]\n",
    "    for j in file_name:\n",
    "        file_path = os.path.join(folder,j)\n",
    "        datalist = []\n",
    "        with open(file_path, 'r') as file:  # 以读取模式打开文件\n",
    "            for line in file:       # 逐行读取文件内容\n",
    "                # 按空格分割每一行，并将每个部分转换为浮点型\n",
    "                float_values = [float(x) for x in line.split()]\n",
    "                datalist.append(float_values)  # 将转换后的浮点型列表添加到 datalist\n",
    "        new_labels = kmeans_loaded.predict(datalist)\n",
    "        all_new_labels.append(new_labels)\n",
    "    save_file_name = os.path.join(save_path,i)+'.npy'\n",
    "    save_data = np.array(all_new_labels,dtype=object)\n",
    "    # print(save_data)\n",
    "    np.save(save_file_name,save_data)\n",
    "    print(save_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a90a5de-df03-491f-97d9-148d997e2a81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../SLR_dataset/kmeans_300_seq_双手合并/000000.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000001.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000002.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000003.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000004.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000005.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000006.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000007.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000008.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000009.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000010.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000011.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000012.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000013.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000014.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000015.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000016.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000017.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000018.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000019.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000020.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000021.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000022.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000023.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000024.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000025.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000026.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000027.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000028.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000029.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000030.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000031.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000032.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000033.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000034.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000035.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000036.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000037.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000038.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000039.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000040.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000041.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000042.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000043.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000044.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000045.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000046.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000047.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000048.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000049.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000050.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000051.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000052.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000053.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000054.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000055.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000056.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000057.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000058.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000059.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000060.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000061.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000062.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000063.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000064.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000065.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000066.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000067.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000068.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000069.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000070.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000071.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000072.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000073.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000074.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000075.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000076.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000077.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000078.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000079.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000080.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000081.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000082.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000083.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000084.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000085.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000086.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000087.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000088.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000089.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000090.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000091.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000092.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000093.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000094.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000095.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000096.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000097.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000098.npy\n",
      "../SLR_dataset/kmeans_300_seq_双手合并/000099.npy\n"
     ]
    }
   ],
   "source": [
    "for i in all_folder:\n",
    "    all_new_labels = []\n",
    "    folder = os.path.join(all_path,i)\n",
    "    file_name = [file for file in os.listdir(folder) if file.endswith('.avi')]\n",
    "    for j in file_name:\n",
    "        file_path = os.path.join(folder,j)\n",
    "        datalist = []\n",
    "        with open(file_path, 'r') as file:  # 以读取模式打开文件\n",
    "            for line in file:       # 逐行读取文件内容\n",
    "                # 按空格分割每一行，并将每个部分转换为浮点型\n",
    "                float_values = [float(x) for x in line.split()]\n",
    "                datalist.append(float_values)  # 将转换后的浮点型列表添加到 datalist\n",
    "        new_labels = kmeans_loaded.predict(datalist)\n",
    "        all_new_labels.append(new_labels)\n",
    "    save_file_name = os.path.join(save_path,i)+'.npy'\n",
    "    save_data = np.array(all_new_labels,dtype=object)\n",
    "    # print(save_data)\n",
    "    np.save(save_file_name,save_data)\n",
    "    print(save_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f599e94b-eecb-4721-ae02-19d0fcd538da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../SLR_dataset/kmeans_100_seq_双手合并/000000.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m xxx \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../SLR_dataset/kmeans_100_seq_双手合并/000000.npy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/numpy/lib/npyio.py:427\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    425\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 427\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    428\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../SLR_dataset/kmeans_100_seq_双手合并/000000.npy'"
     ]
    }
   ],
   "source": [
    "xxx = np.load('../SLR_dataset/kmeans_100_seq_双手合并/000000.npy',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c580548d-21e2-4314-9fe1-db741fa052e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
