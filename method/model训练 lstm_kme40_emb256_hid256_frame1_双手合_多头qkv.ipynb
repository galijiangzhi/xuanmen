{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33843159-6e52-4628-831b-d910ff7f054a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/opt/conda/lib/python3.10/site-packages/torchtext/utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import numpy as np\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import jieba\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7ad326e-3de0-4c27-9f91-3fd556d8d7ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#参数列表\n",
    "\n",
    "\n",
    "#模型参数\n",
    "input_dim = 42 #输入词汇表大小(等于原词汇表大小+2，+2加的是结束符号和填充符号）\n",
    "emb_dim=256       # 词向量维度\n",
    "hidden_dim=256  # LSTM隐藏层维度\n",
    "output_dim=181   # 输出词汇表大小（需你确认）\n",
    "n_layers=1\n",
    "OUTPUT_DIM=181 # 输出词汇表大小（需你确认）\n",
    "batch_size = 2048\n",
    "\n",
    "savepath = '../model/xuanmen_km40' #模型保存地址\n",
    "savename = 'lstm_kme40_emb256_hid256_frame1_双手合并_多头qkv.pth' #模型保存名称\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "#数据集参数 \n",
    "data_dir = \"../SLR_dataset/kmeans_40_seq_双手合并/\"#数据集源文件根目录\n",
    "max_length = 400  # 源序列最大长度\n",
    "end_token = (input_dim - 2)     # 源序列结束符号\n",
    "pad_token = (input_dim - 1)     # 源填充符号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63c54498-cc9c-4f09-90f0-12131fa88eda",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.923 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词表大小：181\n"
     ]
    }
   ],
   "source": [
    "npy_files = sorted([f for f in os.listdir(data_dir) if f.endswith(\".npy\")])\n",
    "labels = open(\"../SLR_dataset/corpus.txt\").read().splitlines()  # 假设每行是一个标签\n",
    "labels = [i.split()[1] for i in labels]\n",
    "labels = [i.replace('\\ufeff','') for i in labels]\n",
    "samples = [np.load(os.path.join(data_dir, f),allow_pickle=True) for f in npy_files]\n",
    "\n",
    "# 中文按字符分词（如需分词需修改为jieba等）\n",
    "tokenizer = lambda x: list(jieba.cut(x)) \n",
    "\n",
    "# 构建词表（添加特殊标记）\n",
    "def yield_tokens(texts):\n",
    "    for text in texts:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(\n",
    "    yield_tokens(labels), \n",
    "    specials=[\"<start>\",\"<pad>\", \"<unk>\", \"<end>\"]\n",
    ")\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "# 转换为序列并添加<end>标记\n",
    "sequences = [torch.tensor([vocab[\"<start>\"]]+vocab(tokenizer(text)) + [vocab[\"<end>\"]]) for text in labels]\n",
    "\n",
    "# 统一填充长度（填充<pad>）\n",
    "padded_sequences = pad_sequence(\n",
    "    sequences, \n",
    "    batch_first=True, \n",
    "    padding_value=vocab[\"<pad>\"]\n",
    ")\n",
    "idx2word = vocab.get_itos()\n",
    "\n",
    "all_data = []\n",
    "for i in range(len(samples)):\n",
    "    input_seq = padded_sequences[i]  # 获取对应的输入序列\n",
    "    \n",
    "    for j in samples[i]:\n",
    "        # 将j转为tensor（如果不是）\n",
    "        j_tensor = torch.tensor(j) if not isinstance(j, torch.Tensor) else j.clone().detach()\n",
    "        \n",
    "        # 1. 先添加结束符41（计入1500长度内）\n",
    "        j_with_end = torch.cat([j_tensor, torch.tensor([end_token], dtype=j_tensor.dtype)])\n",
    "        \n",
    "        # 2. 处理长度\n",
    "        if len(j_with_end) > max_length:\n",
    "            # 如果超长：截断到1499再加结束符\n",
    "            j_processed = torch.cat([j_with_end[:max_length-1], \n",
    "                                   torch.tensor([end_token], dtype=j_tensor.dtype)])\n",
    "        elif len(j_with_end) < max_length:\n",
    "            pad_needed = max_length - len(j_with_end)\n",
    "            padding = torch.full((pad_needed,), pad_token, dtype=j_tensor.dtype)\n",
    "            j_processed = torch.cat([j_with_end, padding])\n",
    "        else:\n",
    "            # 刚好1500\n",
    "            j_processed = j_with_end\n",
    "        \n",
    "        # 验证长度\n",
    "        assert len(j_processed) == max_length, f\"长度错误：{len(j_processed)} != {max_length}\"\n",
    "        \n",
    "        # 添加到最终数据\n",
    "        all_data.append([input_seq, j_processed])\n",
    "        \n",
    "print(f'词表大小：{len(vocab.get_stoi())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "851694f0-91db-4d75-8957-477aca740da9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.data =all_data \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx][1]\n",
    "        label = self.data[idx][0]\n",
    "        return sample, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ce3f728-720b-429f-ae05-4a78b1e30815",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = CustomDataset()\n",
    "# 定义划分比例（例如80%训练，20%测试）\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "# 随机划分\n",
    "train_dataset, test_dataset = random_split(\n",
    "    dataset, \n",
    "    [train_size, test_size],\n",
    "    generator=torch.Generator().manual_seed(42)  # 固定随机种子确保可复现\n",
    ")\n",
    "\n",
    "# 创建DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e22ce0c-536e-4d19-ac83-e9d1edca584b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class seq2seq(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hidden_dim,output_dim,n_layers):\n",
    "        super().__init__()\n",
    "        self.encode_embedding = nn.Embedding(input_dim, emb_dim) #将每个词扩充为emb_dim维\n",
    "        self.decode_embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.encode = nn.LSTM(emb_dim, hidden_dim, n_layers)\n",
    "        self.decode = nn.LSTM(emb_dim,hidden_dim, n_layers)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    def forward(self, src, tar):\n",
    "        # src: [batch_size, src_len]\n",
    "        # tar: [batch_size, trg_len]\n",
    "        \n",
    "        # 编码器部分\n",
    "        encode_embedded = self.encode_embedding(src)  # [batch_size, src_len, emb_dim]\n",
    "        encode_embedded = encode_embedded.permute(1, 0, 2)  # [src_len, batch_size, emb_dim]\n",
    "        _, (hidden, cell) = self.encode(encode_embedded)\n",
    "        \n",
    "        # 解码器部分\n",
    "        batch_size = tar.shape[0] #3\n",
    "        trg_len = tar.shape[1] #9\n",
    "        output_dim = self.fc.out_features #181\n",
    "        # print(output_dim)\n",
    "        \n",
    "        # 准备输出张量\n",
    "        outputs = torch.zeros(trg_len, batch_size, output_dim).to(src.device)#9x3x181\n",
    "        \n",
    "        # 初始输入是<sos> token，这里假设tar已经包含<sos>作为第一个token\n",
    "        input = tar[:, 0]  # 取第一个token作为初始输入 [batch_size]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            # 嵌入输入\n",
    "            embedded = self.decode_embedding(input).unsqueeze(0)  # [1, batch_size, emb_dim]\n",
    "            # print(f'embedded:{embedded.size()}')\n",
    "            # print(f'hidden:{hidden.size()}')\n",
    "            # 通过解码器\n",
    "            output, (hidden, cell) = self.decode(embedded, (hidden, cell))\n",
    "            \n",
    "            # 预测下一个token\n",
    "            pred = self.fc(output.squeeze(0))\n",
    "            outputs[t] = pred\n",
    "            \n",
    "            # 下一个输入是真实目标(teacher forcing)或预测结果\n",
    "            # 这里使用teacher forcing，传入真实目标\n",
    "            input = tar[:, t]\n",
    "        \n",
    "        return outputs.permute(1, 0, 2)  # [batch_size, trg_len, output_dim]\n",
    "    def predict(self, src, sos_token_idx=0, eos_token_idx=1, max_len=9):\n",
    "        \"\"\"\n",
    "        自回归预测（不需要输入tar）\n",
    "        :param src: 输入序列 [batch_size, src_len]\n",
    "        :param sos_token_idx: <sos>的索引\n",
    "        :param eos_token_idx: <eos>的索引（可选）\n",
    "        :param max_len: 最大生成长度\n",
    "        :return: 预测序列 [batch_size, max_len]\n",
    "        \"\"\"\n",
    "        # 编码器部分\n",
    "        encode_embedded = self.encode_embedding(src).permute(1, 0, 2)\n",
    "        _, (hidden, cell) = self.encode(encode_embedded)\n",
    "        \n",
    "        # 解码器初始化\n",
    "        batch_size = src.size(0)\n",
    "        outputs = torch.zeros(batch_size, max_len).long().to(src.device)\n",
    "        input = torch.full((batch_size,), sos_token_idx, dtype=torch.long).to(src.device)\n",
    "        \n",
    "        # 自回归解码\n",
    "        for t in range(max_len):\n",
    "            embedded = self.decode_embedding(input).unsqueeze(0)  # [1, batch_size, emb_dim]\n",
    "            output, (hidden, cell) = self.decode(embedded, (hidden, cell))\n",
    "            pred = self.fc(output.squeeze(0)).argmax(-1)  # [batch_size]\n",
    "            \n",
    "            outputs[:, t] = pred\n",
    "            input = pred  # 使用预测结果作为下一输入\n",
    "            \n",
    "            # 如果所有序列都生成<eos>则提前停止\n",
    "            if eos_token_idx is not None and (pred == eos_token_idx).all():\n",
    "                break\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8745aa08-fedd-487a-84f0-033389c86c2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Seq2SeqWithMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hidden_dim, output_dim, n_layers, num_heads):\n",
    "        super(Seq2SeqWithMultiHeadAttention, self).__init__()\n",
    "        self.encode_embedding = nn.Embedding(input_dim, emb_dim)  # 输入嵌入层\n",
    "        self.decode_embedding = nn.Embedding(output_dim, emb_dim)  # 输出嵌入层\n",
    "        self.encoder = nn.LSTM(emb_dim, hidden_dim, n_layers, batch_first=False)\n",
    "        self.decoder = nn.LSTM(emb_dim + hidden_dim, hidden_dim, n_layers, batch_first=False)\n",
    "        \n",
    "        # 多头注意力相关参数\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        assert hidden_dim % num_heads == 0, \"Hidden dimension must be divisible by the number of heads\"\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "        \n",
    "        # 注意力权重矩阵\n",
    "        self.W_q = nn.Linear(hidden_dim, hidden_dim)  # Query 线性变换\n",
    "        self.W_k = nn.Linear(hidden_dim, hidden_dim)  # Key 线性变换\n",
    "        self.W_v = nn.Linear(hidden_dim, hidden_dim)  # Value 线性变换\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def attention(self, hidden, encoder_outputs):\n",
    "        \"\"\"\n",
    "        多头注意力机制\n",
    "        :param hidden: 解码器的隐藏状态 [batch_size, hidden_dim]\n",
    "        :param encoder_outputs: 编码器的所有隐藏状态 [src_len, batch_size, hidden_dim]\n",
    "        :return: 上下文向量 [batch_size, hidden_dim], 注意力权重 [batch_size, src_len]\n",
    "        \"\"\"\n",
    "        batch_size = hidden.size(0)\n",
    "        src_len = encoder_outputs.size(0)\n",
    "\n",
    "        # 将编码器输出和解码器隐藏状态映射到 Q、K、V\n",
    "        Q = self.W_q(hidden).view(batch_size, 1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)  # [batch_size, num_heads, 1, head_dim]\n",
    "        K = self.W_k(encoder_outputs.permute(1, 0, 2)).view(batch_size, src_len, self.num_heads, self.head_dim).permute(0, 2, 1, 3)  # [batch_size, num_heads, src_len, head_dim]\n",
    "        V = self.W_v(encoder_outputs.permute(1, 0, 2)).view(batch_size, src_len, self.num_heads, self.head_dim).permute(0, 2, 1, 3)  # [batch_size, num_heads, src_len, head_dim]\n",
    "\n",
    "        # 计算点积注意力分数\n",
    "        energy = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)  # [batch_size, num_heads, 1, src_len]\n",
    "        attn_weights = F.softmax(energy, dim=-1)  # [batch_size, num_heads, 1, src_len]\n",
    "\n",
    "        # 加权求和得到上下文向量\n",
    "        context = torch.matmul(attn_weights, V)  # [batch_size, num_heads, 1, head_dim]\n",
    "        context = context.permute(0, 2, 1, 3).contiguous().view(batch_size, 1, self.hidden_dim)  # [batch_size, 1, hidden_dim]\n",
    "        context = context.squeeze(1)  # [batch_size, hidden_dim]\n",
    "\n",
    "        return context, attn_weights\n",
    "\n",
    "    def forward(self, src, tar):\n",
    "        # src: [batch_size, src_len]\n",
    "        # tar: [batch_size, trg_len]\n",
    "\n",
    "        # 编码器部分\n",
    "        encode_embedded = self.encode_embedding(src)  # [batch_size, src_len, emb_dim]\n",
    "        encode_embedded = encode_embedded.permute(1, 0, 2)  # [src_len, batch_size, emb_dim]\n",
    "        encoder_outputs, (hidden, cell) = self.encoder(encode_embedded)  # encoder_outputs: [src_len, batch_size, hidden_dim]\n",
    "\n",
    "        # 解码器部分\n",
    "        batch_size = tar.shape[0]\n",
    "        trg_len = tar.shape[1]\n",
    "        output_dim = self.fc.out_features\n",
    "\n",
    "        # 准备输出张量\n",
    "        outputs = torch.zeros(trg_len, batch_size, output_dim).to(src.device)  # [trg_len, batch_size, output_dim]\n",
    "\n",
    "        # 初始输入是<sos> token\n",
    "        input = tar[:, 0]  # 取第一个token作为初始输入 [batch_size]\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "            # 嵌入输入\n",
    "            embedded = self.decode_embedding(input).unsqueeze(0)  # [1, batch_size, emb_dim]\n",
    "\n",
    "            # 计算多头注意力上下文向量\n",
    "            context, _ = self.attention(hidden[-1], encoder_outputs)  # context: [batch_size, hidden_dim]\n",
    "\n",
    "            # 将上下文向量与嵌入输入拼接\n",
    "            rnn_input = torch.cat((embedded, context.unsqueeze(0)), dim=2)  # [1, batch_size, emb_dim + hidden_dim]\n",
    "\n",
    "            # 通过解码器\n",
    "            output, (hidden, cell) = self.decoder(rnn_input, (hidden, cell))\n",
    "\n",
    "            # 预测下一个token\n",
    "            pred = self.fc(output.squeeze(0))  # [batch_size, output_dim]\n",
    "            outputs[t] = pred\n",
    "\n",
    "            # 下一个输入是真实目标(teacher forcing)或预测结果\n",
    "            input = tar[:, t]  # 使用真实目标\n",
    "\n",
    "        return outputs.permute(1, 0, 2)  # [batch_size, trg_len, output_dim]\n",
    "\n",
    "    def predict(self, src, sos_token_idx=0, eos_token_idx=1, max_len=9):\n",
    "        \"\"\"\n",
    "        自回归预测（不需要输入tar）\n",
    "        :param src: 输入序列 [batch_size, src_len]\n",
    "        :param sos_token_idx: <sos>的索引\n",
    "        :param eos_token_idx: <eos>的索引（可选）\n",
    "        :param max_len: 最大生成长度\n",
    "        :return: 预测序列 [batch_size, max_len]\n",
    "        \"\"\"\n",
    "        # 编码器部分\n",
    "        encode_embedded = self.encode_embedding(src).permute(1, 0, 2)  # [src_len, batch_size, emb_dim]\n",
    "        encoder_outputs, (hidden, cell) = self.encoder(encode_embedded)  # encoder_outputs: [src_len, batch_size, hidden_dim]\n",
    "\n",
    "        # 解码器初始化\n",
    "        batch_size = src.size(0)\n",
    "        outputs = torch.zeros(batch_size, max_len).long().to(src.device)\n",
    "        input = torch.full((batch_size,), sos_token_idx, dtype=torch.long).to(src.device)\n",
    "\n",
    "        for t in range(max_len):\n",
    "            # 嵌入输入\n",
    "            embedded = self.decode_embedding(input).unsqueeze(0)  # [1, batch_size, emb_dim]\n",
    "\n",
    "            # 计算多头注意力上下文向量\n",
    "            context, _ = self.attention(hidden[-1], encoder_outputs)  # context: [batch_size, hidden_dim]\n",
    "\n",
    "            # 将上下文向量与嵌入输入拼接\n",
    "            rnn_input = torch.cat((embedded, context.unsqueeze(0)), dim=2)  # [1, batch_size, emb_dim + hidden_dim]\n",
    "\n",
    "            # 通过解码器\n",
    "            output, (hidden, cell) = self.decoder(rnn_input, (hidden, cell))\n",
    "\n",
    "            # 预测下一个token\n",
    "            pred = self.fc(output.squeeze(0)).argmax(-1)  # [batch_size]\n",
    "\n",
    "            outputs[:, t] = pred\n",
    "            input = pred  # 使用预测结果作为下一输入\n",
    "\n",
    "            # 如果所有序列都生成<eos>则提前停止\n",
    "            if eos_token_idx is not None and (pred == eos_token_idx).all():\n",
    "                break\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6609a332-94c0-4029-ab4d-8a8ab37fa6d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_f1(reference, candidate):\n",
    "    # 统计匹配词数\n",
    "    common_terms = set(reference[0]) & set(candidate)\n",
    "    tp = len(common_terms)  # True Positives\n",
    "    fp = len(candidate) - tp  # False Positives\n",
    "    fn = len(reference[0]) - tp  # False Negatives\n",
    "\n",
    "    # 计算精确度、召回率、F1\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    return precision, recall, f1\n",
    "\n",
    "# precision, recall, f1 = calculate_f1(reference, candidate)\n",
    "# print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a00e991b-1f9e-4626-9017-fcd971a8f9c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, criterion, epochs, device,max_bleu,test_bool=False):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    max_bleu = max_bleu\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for batch_idx, (src, trg) in enumerate(train_loader):\n",
    "            src = src.to(device)  # [batch_size, 424]\n",
    "            trg = trg.to(device)  # [batch_size, 10]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # 前向传播（模型自动处理teacher forcing）\n",
    "            output = model(src, trg)  # [batch_size, 10, OUTPUT_DIM]\n",
    "            \n",
    "            # 计算损失（忽略<sos>和padding）\n",
    "            output = output[:, 1:].reshape(-1, OUTPUT_DIM)  # 忽略<sos>，形状变为[batch_size*9, OUTPUT_DIM]\n",
    "            trg = trg[:, 1:].reshape(-1)                    # 忽略<sos>，形状变为[batch_size*9]\n",
    "            loss = criterion(output, trg)\n",
    "            # print(output)\n",
    "            # print(output.size())\n",
    "            # print(trg)\n",
    "            # print(trg.size())\n",
    "            # return 0\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'Epoch: {epoch+1:03d} | Batch: {batch_idx:03d} | Loss: {loss.item():.4f}')\n",
    "        test_num = 0\n",
    "        test_bleu = float(0)\n",
    "        test_loss = 0\n",
    "        test_jingque = 0\n",
    "        test_zhaohui = 0\n",
    "        test_f1 = 0\n",
    "        if test_bool:\n",
    "            for batch_idx , (src,trg) in enumerate(test_loader):\n",
    "                # print(src)\n",
    "                # print(trg)\n",
    "                src = src.to(device)\n",
    "                trg = trg.to('cpu').tolist()\n",
    "                # print('0000000000000',src)\n",
    "                output = model.predict(src)\n",
    "                # print(output.size())\n",
    "                output = output.tolist()\n",
    "                for i in range(len(output)):\n",
    "                    out = [0]+output[i][:8]\n",
    "                    # print(out)\n",
    "                    tlist = [trg[i]]\n",
    "                    # print(tlist)\n",
    "                    score = sentence_bleu(tlist, out, weights=(0.5, 0.5))\n",
    "                    test_bleu += score\n",
    "                    test_num += 1\n",
    "                    # print(score)\n",
    "                # print('1111111111111',output)\n",
    "                # print('11111111111111111',[0]+output[:8])\n",
    "                # print('222222222222',trg.tolist())\n",
    "                # score = sentence_bleu(trg.tolist(), [0]+output[:8], weights=(0.5, 0.5)) \n",
    "                # precision, recall, f1 = calculate_f1(trg.tolist(), [0]+output[:8])\n",
    "                # test_jingque += precision\n",
    "                # test_zhaohui += recall\n",
    "                # test_f1 += f1\n",
    "        print(f'Epoch: {epoch+1:03d} | Avg Loss: {epoch_loss/len(train_loader):.4f}')\n",
    "        if test_bool:\n",
    "            # print(f'test bleu = {test_bleu/test_num},精确率: {(test_jingque/test_num):.4f}, 召回率: {(test_zhaohui/test_num):.4f}, F1: {(test_f1/test_num):.4f}')\n",
    "            if (test_bleu/test_num) > max_bleu:\n",
    "                max_bleu = (test_bleu/test_num)\n",
    "                \n",
    "                if not os.path.exists(savepath):\n",
    "                    os.makedirs(savepath)\n",
    "                    print(f\"目录已创建：{savepath}\")\n",
    "                else:\n",
    "                    print(f\"目录已存在：{savepath}\")\n",
    "                torch.save(model,os.path.join(savepath,savename))\n",
    "                print(os.path.join(savepath,savename))\n",
    "        print(f'max_bleu={max_bleu}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf162b22-e720-426b-add9-f6188f97ad36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Seq2SeqWithMultiHeadAttention(\n",
    "    input_dim=input_dim,      # 输入词汇表大小\n",
    "    emb_dim=emb_dim,       # 词向量维度\n",
    "    hidden_dim=hidden_dim,    # LSTM隐藏层维度\n",
    "    output_dim=output_dim,     # 输出词汇表大小（需你确认）\n",
    "    n_layers=2,\n",
    "    num_heads=8\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95c6986b-6348-4a34-9e0d-6731d822f777",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model=torch.load(os.path.join(savepath,savename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "da4f0766-e987-4cfd-8279-eec09ad3e02d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2SeqWithMultiHeadAttention(\n",
      "  (encode_embedding): Embedding(42, 256)\n",
      "  (decode_embedding): Embedding(181, 256)\n",
      "  (encoder): LSTM(256, 256, num_layers=2)\n",
      "  (decoder): LSTM(512, 256, num_layers=2)\n",
      "  (W_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (W_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (W_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (fc): Linear(in_features=256, out_features=181, bias=True)\n",
      ")\n",
      "Epoch: 001 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 001 | Avg Loss: 0.0030\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 002 | Batch: 000 | Loss: 0.0041\n",
      "Epoch: 002 | Avg Loss: 0.0025\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 003 | Batch: 000 | Loss: 0.0012\n",
      "Epoch: 003 | Avg Loss: 0.0023\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 004 | Batch: 000 | Loss: 0.0021\n",
      "Epoch: 004 | Avg Loss: 0.0017\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 005 | Batch: 000 | Loss: 0.0021\n",
      "Epoch: 005 | Avg Loss: 0.0009\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 006 | Batch: 000 | Loss: 0.0003\n",
      "Epoch: 006 | Avg Loss: 0.0010\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 007 | Batch: 000 | Loss: 0.0007\n",
      "Epoch: 007 | Avg Loss: 0.0011\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 008 | Batch: 000 | Loss: 0.0003\n",
      "Epoch: 008 | Avg Loss: 0.0008\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 009 | Batch: 000 | Loss: 0.0006\n",
      "Epoch: 009 | Avg Loss: 0.0007\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 010 | Batch: 000 | Loss: 0.0004\n",
      "Epoch: 010 | Avg Loss: 0.0006\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 011 | Batch: 000 | Loss: 0.0006\n",
      "Epoch: 011 | Avg Loss: 0.0012\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 012 | Batch: 000 | Loss: 0.0006\n",
      "Epoch: 012 | Avg Loss: 0.0011\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 013 | Batch: 000 | Loss: 0.0041\n",
      "Epoch: 013 | Avg Loss: 0.0014\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 014 | Batch: 000 | Loss: 0.0007\n",
      "Epoch: 014 | Avg Loss: 0.0015\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 015 | Batch: 000 | Loss: 0.0016\n",
      "Epoch: 015 | Avg Loss: 0.0017\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 016 | Batch: 000 | Loss: 0.0011\n",
      "Epoch: 016 | Avg Loss: 0.0021\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 017 | Batch: 000 | Loss: 0.0041\n",
      "Epoch: 017 | Avg Loss: 0.0017\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 018 | Batch: 000 | Loss: 0.0006\n",
      "Epoch: 018 | Avg Loss: 0.0006\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 019 | Batch: 000 | Loss: 0.0003\n",
      "Epoch: 019 | Avg Loss: 0.0003\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 020 | Batch: 000 | Loss: 0.0002\n",
      "Epoch: 020 | Avg Loss: 0.0001\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 021 | Batch: 000 | Loss: 0.0001\n",
      "Epoch: 021 | Avg Loss: 0.0001\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 022 | Batch: 000 | Loss: 0.0001\n",
      "Epoch: 022 | Avg Loss: 0.0001\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 023 | Batch: 000 | Loss: 0.0001\n",
      "Epoch: 023 | Avg Loss: 0.0001\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 024 | Batch: 000 | Loss: 0.0001\n",
      "Epoch: 024 | Avg Loss: 0.0001\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 025 | Batch: 000 | Loss: 0.0001\n",
      "Epoch: 025 | Avg Loss: 0.0001\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 026 | Batch: 000 | Loss: 0.0001\n",
      "Epoch: 026 | Avg Loss: 0.0001\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 027 | Batch: 000 | Loss: 0.0001\n",
      "Epoch: 027 | Avg Loss: 0.0001\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 028 | Batch: 000 | Loss: 0.0001\n",
      "Epoch: 028 | Avg Loss: 0.0001\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 029 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 029 | Avg Loss: 0.0001\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 030 | Batch: 000 | Loss: 0.0001\n",
      "Epoch: 030 | Avg Loss: 0.0001\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 031 | Batch: 000 | Loss: 0.0001\n",
      "Epoch: 031 | Avg Loss: 0.0001\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 032 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 032 | Avg Loss: 0.0001\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 033 | Batch: 000 | Loss: 0.0001\n",
      "Epoch: 033 | Avg Loss: 0.0001\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 034 | Batch: 000 | Loss: 0.0001\n",
      "Epoch: 034 | Avg Loss: 0.0001\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 035 | Batch: 000 | Loss: 0.0001\n",
      "Epoch: 035 | Avg Loss: 0.0001\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 036 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 036 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 037 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 037 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 038 | Batch: 000 | Loss: 0.0001\n",
      "Epoch: 038 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 039 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 039 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 040 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 040 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 041 | Batch: 000 | Loss: 0.0001\n",
      "Epoch: 041 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 042 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 042 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 043 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 043 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 044 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 044 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 045 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 045 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 046 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 046 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 047 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 047 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 048 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 048 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 049 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 049 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 050 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 050 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 051 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 051 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 052 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 052 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 053 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 053 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 054 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 054 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 055 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 055 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 056 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 056 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 057 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 057 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 058 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 058 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 059 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 059 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 060 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 060 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 061 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 061 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 062 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 062 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 063 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 063 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 064 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 064 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 065 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 065 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 066 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 066 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 067 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 067 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 068 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 068 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 069 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 069 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 070 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 070 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 071 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 071 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 072 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 072 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 073 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 073 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 074 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 074 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 075 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 075 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 076 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 076 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 077 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 077 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 078 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 078 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 079 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 079 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 080 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 080 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 081 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 081 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 082 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 082 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 083 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 083 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 084 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 084 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 085 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 085 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 086 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 086 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 087 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 087 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 088 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 088 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 089 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 089 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 090 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 090 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 091 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 091 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 092 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 092 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 093 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 093 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 094 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 094 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 095 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 095 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 096 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 096 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 097 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 097 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 098 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 098 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 099 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 099 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 100 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 100 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 101 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 101 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 102 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 102 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 103 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 103 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 104 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 104 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 105 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 105 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 106 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 106 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 107 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 107 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 108 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 108 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 109 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 109 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 110 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 110 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 111 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 111 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 112 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 112 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 113 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 113 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 114 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 114 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 115 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 115 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 116 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 116 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 117 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 117 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 118 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 118 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 119 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 119 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 120 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 120 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 121 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 121 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 122 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 122 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 123 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 123 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 124 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 124 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 125 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 125 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 126 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 126 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 127 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 127 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 128 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 128 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 129 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 129 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 130 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 130 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 131 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 131 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 132 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 132 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 133 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 133 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 134 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 134 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 135 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 135 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 136 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 136 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 137 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 137 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 138 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 138 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 139 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 139 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 140 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 140 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 141 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 141 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 142 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 142 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 143 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 143 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 144 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 144 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 145 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 145 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 146 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 146 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 147 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 147 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 148 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 148 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 149 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 149 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 150 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 150 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 151 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 151 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 152 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 152 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 153 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 153 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 154 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 154 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 155 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 155 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 156 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 156 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 157 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 157 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 158 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 158 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 159 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 159 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 160 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 160 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 161 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 161 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 162 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 162 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 163 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 163 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 164 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 164 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 165 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 165 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 166 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 166 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 167 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 167 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 168 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 168 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 169 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 169 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 170 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 170 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 171 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 171 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 172 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 172 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 173 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 173 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 174 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 174 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 175 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 175 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 176 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 176 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 177 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 177 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 178 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 178 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 179 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 179 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 180 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 180 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 181 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 181 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 182 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 182 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 183 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 183 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 184 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 184 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 185 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 185 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 186 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 186 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 187 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 187 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 188 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 188 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 189 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 189 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 190 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 190 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 191 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 191 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 192 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 192 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 193 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 193 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 194 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 194 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 195 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 195 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 196 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 196 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 197 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 197 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 198 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 198 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 199 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 199 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 200 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 200 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 201 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 201 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 202 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 202 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 203 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 203 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 204 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 204 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 205 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 205 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 206 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 206 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 207 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 207 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 208 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 208 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 209 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 209 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 210 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 210 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 211 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 211 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 212 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 212 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 213 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 213 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 214 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 214 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 215 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 215 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 216 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 216 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 217 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 217 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 218 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 218 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 219 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 219 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 220 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 220 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 221 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 221 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 222 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 222 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 223 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 223 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 224 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 224 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 225 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 225 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 226 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 226 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 227 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 227 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 228 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 228 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 229 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 229 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 230 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 230 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 231 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 231 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 232 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 232 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 233 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 233 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 234 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 234 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 235 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 235 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 236 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 236 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 237 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 237 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 238 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 238 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 239 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 239 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 240 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 240 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 241 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 241 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 242 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 242 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 243 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 243 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 244 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 244 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 245 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 245 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 246 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 246 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 247 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 247 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 248 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 248 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 249 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 249 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 250 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 250 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 251 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 251 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 252 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 252 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 253 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 253 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 254 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 254 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 255 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 255 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 256 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 256 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 257 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 257 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 258 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 258 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 259 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 259 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 260 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 260 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 261 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 261 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 262 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 262 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 263 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 263 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 264 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 264 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 265 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 265 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 266 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 266 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 267 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 267 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 268 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 268 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 269 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 269 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 270 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 270 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 271 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 271 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 272 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 272 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 273 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 273 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 274 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 274 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 275 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 275 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 276 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 276 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 277 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 277 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 278 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 278 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 279 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 279 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 280 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 280 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 281 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 281 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 282 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 282 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 283 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 283 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 284 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 284 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 285 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 285 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 286 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 286 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 287 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 287 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 288 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 288 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 289 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 289 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 290 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 290 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 291 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 291 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 292 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 292 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 293 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 293 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 294 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 294 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 295 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 295 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 296 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 296 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 297 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 297 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 298 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 298 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 299 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 299 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 300 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 300 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 301 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 301 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 302 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 302 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 303 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 303 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 304 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 304 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 305 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 305 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 306 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 306 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 307 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 307 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 308 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 308 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 309 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 309 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 310 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 310 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 311 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 311 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 312 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 312 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 313 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 313 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 314 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 314 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 315 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 315 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 316 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 316 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 317 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 317 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 318 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 318 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 319 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 319 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 320 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 320 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 321 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 321 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 322 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 322 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 323 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 323 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 324 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 324 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 325 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 325 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 326 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 326 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 327 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 327 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 328 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 328 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 329 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 329 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 330 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 330 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 331 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 331 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 332 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 332 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 333 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 333 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 334 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 334 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 335 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 335 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 336 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 336 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 337 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 337 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 338 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 338 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 339 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 339 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 340 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 340 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 341 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 341 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 342 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 342 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 343 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 343 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 344 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 344 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 345 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 345 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 346 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 346 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 347 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 347 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 348 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 348 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 349 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 349 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 350 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 350 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 351 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 351 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 352 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 352 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 353 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 353 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 354 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 354 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 355 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 355 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 356 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 356 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 357 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 357 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 358 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 358 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 359 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 359 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 360 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 360 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 361 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 361 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 362 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 362 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 363 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 363 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 364 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 364 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 365 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 365 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 366 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 366 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 367 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 367 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 368 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 368 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 369 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 369 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 370 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 370 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 371 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 371 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 372 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 372 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 373 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 373 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 374 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 374 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 375 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 375 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 376 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 376 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 377 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 377 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 378 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 378 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 379 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 379 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 380 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 380 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 381 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 381 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 382 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 382 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 383 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 383 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 384 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 384 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 385 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 385 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 386 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 386 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 387 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 387 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 388 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 388 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 389 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 389 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 390 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 390 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 391 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 391 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 392 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 392 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 393 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 393 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 394 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 394 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 395 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 395 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 396 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 396 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 397 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 397 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 398 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 398 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 399 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 399 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 400 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 400 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 401 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 401 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 402 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 402 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 403 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 403 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 404 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 404 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 405 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 405 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 406 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 406 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 407 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 407 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 408 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 408 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 409 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 409 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 410 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 410 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 411 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 411 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 412 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 412 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 413 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 413 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 414 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 414 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 415 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 415 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 416 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 416 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 417 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 417 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 418 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 418 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 419 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 419 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 420 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 420 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 421 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 421 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 422 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 422 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 423 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 423 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 424 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 424 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 425 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 425 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 426 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 426 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 427 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 427 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 428 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 428 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 429 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 429 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 430 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 430 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 431 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 431 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 432 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 432 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 433 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 433 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 434 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 434 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 435 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 435 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 436 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 436 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 437 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 437 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 438 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 438 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 439 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 439 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 440 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 440 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 441 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 441 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 442 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 442 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 443 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 443 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 444 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 444 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 445 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 445 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 446 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 446 | Avg Loss: 0.0000\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 447 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 447 | Avg Loss: 0.2563\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 448 | Batch: 000 | Loss: 0.1674\n",
      "Epoch: 448 | Avg Loss: 0.1094\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 449 | Batch: 000 | Loss: 0.0833\n",
      "Epoch: 449 | Avg Loss: 0.0566\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 450 | Batch: 000 | Loss: 0.0415\n",
      "Epoch: 450 | Avg Loss: 0.0344\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 451 | Batch: 000 | Loss: 0.0253\n",
      "Epoch: 451 | Avg Loss: 0.0245\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 452 | Batch: 000 | Loss: 0.0271\n",
      "Epoch: 452 | Avg Loss: 0.0160\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 453 | Batch: 000 | Loss: 0.0152\n",
      "Epoch: 453 | Avg Loss: 0.0186\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 454 | Batch: 000 | Loss: 0.0117\n",
      "Epoch: 454 | Avg Loss: 0.0158\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 455 | Batch: 000 | Loss: 0.0175\n",
      "Epoch: 455 | Avg Loss: 0.0110\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 456 | Batch: 000 | Loss: 0.0142\n",
      "Epoch: 456 | Avg Loss: 0.0106\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 457 | Batch: 000 | Loss: 0.0050\n",
      "Epoch: 457 | Avg Loss: 0.0081\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 458 | Batch: 000 | Loss: 0.0064\n",
      "Epoch: 458 | Avg Loss: 0.0102\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 459 | Batch: 000 | Loss: 0.0095\n",
      "Epoch: 459 | Avg Loss: 0.0064\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 460 | Batch: 000 | Loss: 0.0070\n",
      "Epoch: 460 | Avg Loss: 0.0042\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 461 | Batch: 000 | Loss: 0.0025\n",
      "Epoch: 461 | Avg Loss: 0.0027\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 462 | Batch: 000 | Loss: 0.0037\n",
      "Epoch: 462 | Avg Loss: 0.0036\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 463 | Batch: 000 | Loss: 0.0039\n",
      "Epoch: 463 | Avg Loss: 0.0052\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 464 | Batch: 000 | Loss: 0.0045\n",
      "Epoch: 464 | Avg Loss: 0.0040\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 465 | Batch: 000 | Loss: 0.0019\n",
      "Epoch: 465 | Avg Loss: 0.0030\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 466 | Batch: 000 | Loss: 0.0033\n",
      "Epoch: 466 | Avg Loss: 0.0040\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 467 | Batch: 000 | Loss: 0.0027\n",
      "Epoch: 467 | Avg Loss: 0.0035\n",
      "max_bleu=0.5885722396528988\n",
      "Epoch: 468 | Batch: 000 | Loss: 0.0053\n",
      "Epoch: 468 | Avg Loss: 0.0031\n",
      "目录已存在：../model/xuanmen_km40\n",
      "../model/xuanmen_km40/lstm_kme40_emb256_hid256_frame1_双手合并_多头qkv.pth\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 469 | Batch: 000 | Loss: 0.0009\n",
      "Epoch: 469 | Avg Loss: 0.0019\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 470 | Batch: 000 | Loss: 0.0046\n",
      "Epoch: 470 | Avg Loss: 0.0024\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 471 | Batch: 000 | Loss: 0.0046\n",
      "Epoch: 471 | Avg Loss: 0.0025\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 472 | Batch: 000 | Loss: 0.0011\n",
      "Epoch: 472 | Avg Loss: 0.0014\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 473 | Batch: 000 | Loss: 0.0008\n",
      "Epoch: 473 | Avg Loss: 0.0007\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 474 | Batch: 000 | Loss: 0.0005\n",
      "Epoch: 474 | Avg Loss: 0.0004\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 475 | Batch: 000 | Loss: 0.0004\n",
      "Epoch: 475 | Avg Loss: 0.0003\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 476 | Batch: 000 | Loss: 0.0002\n",
      "Epoch: 476 | Avg Loss: 0.0002\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 477 | Batch: 000 | Loss: 0.0001\n",
      "Epoch: 477 | Avg Loss: 0.0001\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 478 | Batch: 000 | Loss: 0.0001\n",
      "Epoch: 478 | Avg Loss: 0.0001\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 479 | Batch: 000 | Loss: 0.0001\n",
      "Epoch: 479 | Avg Loss: 0.0001\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 480 | Batch: 000 | Loss: 0.0001\n",
      "Epoch: 480 | Avg Loss: 0.0001\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 481 | Batch: 000 | Loss: 0.0001\n",
      "Epoch: 481 | Avg Loss: 0.0001\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 482 | Batch: 000 | Loss: 0.0001\n",
      "Epoch: 482 | Avg Loss: 0.0001\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 483 | Batch: 000 | Loss: 0.0001\n",
      "Epoch: 483 | Avg Loss: 0.0001\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 484 | Batch: 000 | Loss: 0.0001\n",
      "Epoch: 484 | Avg Loss: 0.0001\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 485 | Batch: 000 | Loss: 0.0001\n",
      "Epoch: 485 | Avg Loss: 0.0001\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 486 | Batch: 000 | Loss: 0.0001\n",
      "Epoch: 486 | Avg Loss: 0.0001\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 487 | Batch: 000 | Loss: 0.0001\n",
      "Epoch: 487 | Avg Loss: 0.0001\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 488 | Batch: 000 | Loss: 0.0001\n",
      "Epoch: 488 | Avg Loss: 0.0001\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 489 | Batch: 000 | Loss: 0.0001\n",
      "Epoch: 489 | Avg Loss: 0.0001\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 490 | Batch: 000 | Loss: 0.0001\n",
      "Epoch: 490 | Avg Loss: 0.0001\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 491 | Batch: 000 | Loss: 0.0001\n",
      "Epoch: 491 | Avg Loss: 0.0001\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 492 | Batch: 000 | Loss: 0.0001\n",
      "Epoch: 492 | Avg Loss: 0.0001\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 493 | Batch: 000 | Loss: 0.0001\n",
      "Epoch: 493 | Avg Loss: 0.0001\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 494 | Batch: 000 | Loss: 0.0001\n",
      "Epoch: 494 | Avg Loss: 0.0001\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 495 | Batch: 000 | Loss: 0.0001\n",
      "Epoch: 495 | Avg Loss: 0.0001\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 496 | Batch: 000 | Loss: 0.0001\n",
      "Epoch: 496 | Avg Loss: 0.0001\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 497 | Batch: 000 | Loss: 0.0001\n",
      "Epoch: 497 | Avg Loss: 0.0001\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 498 | Batch: 000 | Loss: 0.0001\n",
      "Epoch: 498 | Avg Loss: 0.0001\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 499 | Batch: 000 | Loss: 0.0001\n",
      "Epoch: 499 | Avg Loss: 0.0001\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 500 | Batch: 000 | Loss: 0.0001\n",
      "Epoch: 500 | Avg Loss: 0.0001\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 501 | Batch: 000 | Loss: 0.0001\n",
      "Epoch: 501 | Avg Loss: 0.0001\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 502 | Batch: 000 | Loss: 0.0001\n",
      "Epoch: 502 | Avg Loss: 0.0001\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 503 | Batch: 000 | Loss: 0.0001\n",
      "Epoch: 503 | Avg Loss: 0.0001\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 504 | Batch: 000 | Loss: 0.0001\n",
      "Epoch: 504 | Avg Loss: 0.0001\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 505 | Batch: 000 | Loss: 0.0001\n",
      "Epoch: 505 | Avg Loss: 0.0001\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 506 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 506 | Avg Loss: 0.0001\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 507 | Batch: 000 | Loss: 0.0001\n",
      "Epoch: 507 | Avg Loss: 0.0001\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 508 | Batch: 000 | Loss: 0.0001\n",
      "Epoch: 508 | Avg Loss: 0.0001\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 509 | Batch: 000 | Loss: 0.0001\n",
      "Epoch: 509 | Avg Loss: 0.0001\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 510 | Batch: 000 | Loss: 0.0001\n",
      "Epoch: 510 | Avg Loss: 0.0001\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 511 | Batch: 000 | Loss: 0.0001\n",
      "Epoch: 511 | Avg Loss: 0.0001\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 512 | Batch: 000 | Loss: 0.0001\n",
      "Epoch: 512 | Avg Loss: 0.0001\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 513 | Batch: 000 | Loss: 0.0001\n",
      "Epoch: 513 | Avg Loss: 0.0001\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 514 | Batch: 000 | Loss: 0.0001\n",
      "Epoch: 514 | Avg Loss: 0.0001\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 515 | Batch: 000 | Loss: 0.0001\n",
      "Epoch: 515 | Avg Loss: 0.0001\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 516 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 516 | Avg Loss: 0.0001\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 517 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 517 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 518 | Batch: 000 | Loss: 0.0001\n",
      "Epoch: 518 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 519 | Batch: 000 | Loss: 0.0001\n",
      "Epoch: 519 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 520 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 520 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 521 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 521 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 522 | Batch: 000 | Loss: 0.0001\n",
      "Epoch: 522 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 523 | Batch: 000 | Loss: 0.0001\n",
      "Epoch: 523 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 524 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 524 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 525 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 525 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 526 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 526 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 527 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 527 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 528 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 528 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 529 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 529 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 530 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 530 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 531 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 531 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 532 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 532 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 533 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 533 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 534 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 534 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 535 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 535 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 536 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 536 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 537 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 537 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 538 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 538 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 539 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 539 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 540 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 540 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 541 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 541 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 542 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 542 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 543 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 543 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 544 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 544 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 545 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 545 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 546 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 546 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 547 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 547 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 548 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 548 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 549 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 549 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 550 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 550 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 551 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 551 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 552 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 552 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 553 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 553 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 554 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 554 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 555 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 555 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 556 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 556 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 557 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 557 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 558 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 558 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 559 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 559 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 560 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 560 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 561 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 561 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 562 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 562 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 563 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 563 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 564 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 564 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 565 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 565 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 566 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 566 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 567 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 567 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 568 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 568 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 569 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 569 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 570 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 570 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 571 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 571 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 572 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 572 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 573 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 573 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 574 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 574 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 575 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 575 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 576 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 576 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 577 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 577 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 578 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 578 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 579 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 579 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 580 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 580 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 581 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 581 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 582 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 582 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 583 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 583 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 584 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 584 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 585 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 585 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 586 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 586 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 587 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 587 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 588 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 588 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 589 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 589 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 590 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 590 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 591 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 591 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 592 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 592 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 593 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 593 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 594 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 594 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 595 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 595 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 596 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 596 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 597 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 597 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 598 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 598 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 599 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 599 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 600 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 600 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 601 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 601 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 602 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 602 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 603 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 603 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 604 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 604 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 605 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 605 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 606 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 606 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 607 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 607 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 608 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 608 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 609 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 609 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 610 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 610 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 611 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 611 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 612 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 612 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 613 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 613 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 614 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 614 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 615 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 615 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 616 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 616 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 617 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 617 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 618 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 618 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 619 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 619 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 620 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 620 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 621 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 621 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 622 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 622 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 623 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 623 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 624 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 624 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 625 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 625 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 626 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 626 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 627 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 627 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 628 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 628 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 629 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 629 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 630 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 630 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 631 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 631 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 632 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 632 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 633 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 633 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 634 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 634 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 635 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 635 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 636 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 636 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 637 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 637 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 638 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 638 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 639 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 639 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 640 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 640 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 641 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 641 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 642 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 642 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 643 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 643 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 644 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 644 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 645 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 645 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 646 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 646 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 647 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 647 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 648 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 648 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 649 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 649 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 650 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 650 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 651 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 651 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 652 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 652 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 653 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 653 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 654 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 654 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 655 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 655 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 656 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 656 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 657 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 657 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 658 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 658 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 659 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 659 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 660 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 660 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 661 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 661 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 662 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 662 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 663 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 663 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 664 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 664 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 665 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 665 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 666 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 666 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 667 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 667 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 668 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 668 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 669 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 669 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 670 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 670 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 671 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 671 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 672 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 672 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 673 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 673 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 674 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 674 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 675 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 675 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 676 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 676 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 677 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 677 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 678 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 678 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 679 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 679 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 680 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 680 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 681 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 681 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 682 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 682 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 683 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 683 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 684 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 684 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 685 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 685 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 686 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 686 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 687 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 687 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 688 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 688 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 689 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 689 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 690 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 690 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 691 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 691 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 692 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 692 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 693 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 693 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 694 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 694 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 695 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 695 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 696 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 696 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 697 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 697 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 698 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 698 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 699 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 699 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 700 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 700 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 701 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 701 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 702 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 702 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 703 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 703 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 704 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 704 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 705 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 705 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 706 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 706 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 707 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 707 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 708 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 708 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 709 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 709 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 710 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 710 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 711 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 711 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 712 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 712 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 713 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 713 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 714 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 714 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 715 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 715 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 716 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 716 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 717 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 717 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 718 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 718 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 719 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 719 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 720 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 720 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 721 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 721 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 722 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 722 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 723 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 723 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 724 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 724 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 725 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 725 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 726 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 726 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 727 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 727 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 728 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 728 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 729 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 729 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 730 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 730 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 731 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 731 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 732 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 732 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 733 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 733 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 734 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 734 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 735 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 735 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 736 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 736 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 737 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 737 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 738 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 738 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 739 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 739 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 740 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 740 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 741 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 741 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 742 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 742 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 743 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 743 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 744 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 744 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 745 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 745 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 746 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 746 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 747 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 747 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 748 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 748 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 749 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 749 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 750 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 750 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 751 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 751 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 752 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 752 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 753 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 753 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 754 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 754 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 755 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 755 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 756 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 756 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 757 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 757 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 758 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 758 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 759 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 759 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 760 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 760 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 761 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 761 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 762 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 762 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 763 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 763 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 764 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 764 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 765 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 765 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 766 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 766 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 767 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 767 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 768 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 768 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 769 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 769 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 770 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 770 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 771 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 771 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 772 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 772 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 773 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 773 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 774 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 774 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 775 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 775 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 776 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 776 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 777 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 777 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 778 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 778 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 779 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 779 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 780 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 780 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 781 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 781 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 782 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 782 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 783 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 783 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 784 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 784 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 785 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 785 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 786 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 786 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 787 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 787 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 788 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 788 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 789 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 789 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 790 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 790 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 791 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 791 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 792 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 792 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 793 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 793 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 794 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 794 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 795 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 795 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 796 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 796 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 797 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 797 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 798 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 798 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 799 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 799 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 800 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 800 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 801 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 801 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 802 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 802 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 803 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 803 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 804 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 804 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 805 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 805 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 806 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 806 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 807 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 807 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 808 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 808 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 809 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 809 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 810 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 810 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 811 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 811 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 812 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 812 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 813 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 813 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 814 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 814 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 815 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 815 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 816 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 816 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 817 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 817 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 818 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 818 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 819 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 819 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 820 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 820 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 821 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 821 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 822 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 822 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 823 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 823 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 824 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 824 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 825 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 825 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 826 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 826 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 827 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 827 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 828 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 828 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 829 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 829 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 830 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 830 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 831 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 831 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 832 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 832 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 833 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 833 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 834 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 834 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 835 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 835 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 836 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 836 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 837 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 837 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 838 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 838 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 839 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 839 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 840 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 840 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 841 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 841 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 842 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 842 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 843 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 843 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 844 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 844 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 845 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 845 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 846 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 846 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 847 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 847 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 848 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 848 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 849 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 849 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 850 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 850 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 851 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 851 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 852 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 852 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 853 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 853 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 854 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 854 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 855 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 855 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 856 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 856 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 857 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 857 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 858 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 858 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 859 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 859 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 860 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 860 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 861 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 861 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 862 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 862 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 863 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 863 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 864 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 864 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 865 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 865 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 866 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 866 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 867 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 867 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 868 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 868 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 869 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 869 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 870 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 870 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 871 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 871 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 872 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 872 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 873 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 873 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 874 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 874 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 875 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 875 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 876 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 876 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 877 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 877 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 878 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 878 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 879 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 879 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 880 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 880 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 881 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 881 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 882 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 882 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 883 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 883 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 884 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 884 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 885 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 885 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 886 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 886 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 887 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 887 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 888 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 888 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 889 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 889 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 890 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 890 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 891 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 891 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 892 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 892 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 893 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 893 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 894 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 894 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 895 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 895 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 896 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 896 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 897 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 897 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 898 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 898 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 899 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 899 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 900 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 900 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 901 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 901 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 902 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 902 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 903 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 903 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 904 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 904 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 905 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 905 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 906 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 906 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 907 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 907 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 908 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 908 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 909 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 909 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 910 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 910 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 911 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 911 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 912 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 912 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 913 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 913 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 914 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 914 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 915 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 915 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 916 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 916 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 917 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 917 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 918 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 918 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 919 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 919 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 920 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 920 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 921 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 921 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 922 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 922 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 923 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 923 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 924 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 924 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 925 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 925 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 926 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 926 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 927 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 927 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 928 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 928 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 929 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 929 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 930 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 930 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 931 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 931 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 932 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 932 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 933 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 933 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 934 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 934 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 935 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 935 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 936 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 936 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 937 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 937 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 938 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 938 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 939 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 939 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 940 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 940 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 941 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 941 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 942 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 942 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 943 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 943 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 944 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 944 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 945 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 945 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 946 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 946 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 947 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 947 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 948 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 948 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 949 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 949 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 950 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 950 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 951 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 951 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 952 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 952 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 953 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 953 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 954 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 954 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 955 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 955 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 956 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 956 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 957 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 957 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 958 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 958 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 959 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 959 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 960 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 960 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 961 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 961 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 962 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 962 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 963 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 963 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 964 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 964 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 965 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 965 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 966 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 966 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 967 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 967 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 968 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 968 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 969 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 969 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 970 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 970 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 971 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 971 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 972 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 972 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 973 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 973 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 974 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 974 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 975 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 975 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 976 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 976 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 977 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 977 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 978 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 978 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 979 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 979 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 980 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 980 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 981 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 981 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 982 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 982 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 983 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 983 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 984 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 984 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 985 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 985 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 986 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 986 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 987 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 987 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 988 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 988 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 989 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 989 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 990 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 990 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 991 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 991 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 992 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 992 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 993 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 993 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 994 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 994 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 995 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 995 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 996 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 996 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 997 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 997 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 998 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 998 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 999 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 999 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1000 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1000 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1001 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1001 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1002 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1002 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1003 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1003 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1004 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1004 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1005 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1005 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1006 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1006 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1007 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1007 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1008 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1008 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1009 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1009 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1010 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1010 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1011 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1011 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1012 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1012 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1013 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1013 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1014 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1014 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1015 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1015 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1016 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1016 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1017 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1017 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1018 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1018 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1019 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1019 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1020 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1020 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1021 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1021 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1022 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1022 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1023 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1023 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1024 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1024 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1025 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1025 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1026 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1026 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1027 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1027 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1028 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1028 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1029 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1029 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1030 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1030 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1031 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1031 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1032 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1032 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1033 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1033 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1034 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1034 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1035 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1035 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1036 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1036 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1037 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1037 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1038 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1038 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1039 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1039 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1040 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1040 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1041 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1041 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1042 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1042 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1043 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1043 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1044 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1044 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1045 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1045 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1046 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1046 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1047 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1047 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1048 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1048 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1049 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1049 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1050 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1050 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1051 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1051 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1052 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1052 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1053 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1053 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1054 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1054 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1055 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1055 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1056 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1056 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1057 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1057 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1058 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1058 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1059 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1059 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1060 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1060 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1061 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1061 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1062 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1062 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1063 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1063 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1064 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1064 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1065 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1065 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1066 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1066 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1067 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1067 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1068 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1068 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1069 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1069 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1070 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1070 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1071 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1071 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1072 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1072 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1073 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1073 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1074 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1074 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1075 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1075 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1076 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1076 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1077 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1077 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1078 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1078 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1079 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1079 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1080 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1080 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1081 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1081 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1082 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1082 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1083 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1083 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1084 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1084 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1085 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1085 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1086 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1086 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1087 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1087 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1088 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1088 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1089 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1089 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1090 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1090 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1091 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1091 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1092 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1092 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1093 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1093 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1094 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1094 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1095 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1095 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1096 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1096 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1097 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1097 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1098 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1098 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1099 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1099 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1100 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1100 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1101 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1101 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1102 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1102 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1103 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1103 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1104 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1104 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1105 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1105 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1106 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1106 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1107 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1107 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1108 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1108 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1109 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1109 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1110 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1110 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1111 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1111 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1112 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1112 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1113 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1113 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1114 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1114 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1115 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1115 | Avg Loss: 0.0000\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1116 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1116 | Avg Loss: 0.1355\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1117 | Batch: 000 | Loss: 0.1318\n",
      "Epoch: 1117 | Avg Loss: 0.1590\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1118 | Batch: 000 | Loss: 0.0654\n",
      "Epoch: 1118 | Avg Loss: 0.0507\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1119 | Batch: 000 | Loss: 0.0235\n",
      "Epoch: 1119 | Avg Loss: 0.0251\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1120 | Batch: 000 | Loss: 0.0204\n",
      "Epoch: 1120 | Avg Loss: 0.0190\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1121 | Batch: 000 | Loss: 0.0189\n",
      "Epoch: 1121 | Avg Loss: 0.0176\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1122 | Batch: 000 | Loss: 0.0121\n",
      "Epoch: 1122 | Avg Loss: 0.0137\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1123 | Batch: 000 | Loss: 0.0101\n",
      "Epoch: 1123 | Avg Loss: 0.0098\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1124 | Batch: 000 | Loss: 0.0129\n",
      "Epoch: 1124 | Avg Loss: 0.0101\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1125 | Batch: 000 | Loss: 0.0207\n",
      "Epoch: 1125 | Avg Loss: 0.0115\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1126 | Batch: 000 | Loss: 0.0063\n",
      "Epoch: 1126 | Avg Loss: 0.0060\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1127 | Batch: 000 | Loss: 0.0063\n",
      "Epoch: 1127 | Avg Loss: 0.0058\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1128 | Batch: 000 | Loss: 0.0067\n",
      "Epoch: 1128 | Avg Loss: 0.0059\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1129 | Batch: 000 | Loss: 0.0032\n",
      "Epoch: 1129 | Avg Loss: 0.0045\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1130 | Batch: 000 | Loss: 0.0031\n",
      "Epoch: 1130 | Avg Loss: 0.0047\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1131 | Batch: 000 | Loss: 0.0063\n",
      "Epoch: 1131 | Avg Loss: 0.0040\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1132 | Batch: 000 | Loss: 0.0061\n",
      "Epoch: 1132 | Avg Loss: 0.0035\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1133 | Batch: 000 | Loss: 0.0020\n",
      "Epoch: 1133 | Avg Loss: 0.0022\n",
      "max_bleu=0.590894612164138\n",
      "Epoch: 1134 | Batch: 000 | Loss: 0.0022\n",
      "Epoch: 1134 | Avg Loss: 0.0022\n",
      "目录已存在：../model/xuanmen_km40\n",
      "../model/xuanmen_km40/lstm_kme40_emb256_hid256_frame1_双手合并_多头qkv.pth\n",
      "max_bleu=0.5915666585834488\n",
      "Epoch: 1135 | Batch: 000 | Loss: 0.0021\n",
      "Epoch: 1135 | Avg Loss: 0.0023\n",
      "max_bleu=0.5915666585834488\n",
      "Epoch: 1136 | Batch: 000 | Loss: 0.0030\n",
      "Epoch: 1136 | Avg Loss: 0.0023\n",
      "max_bleu=0.5915666585834488\n",
      "Epoch: 1137 | Batch: 000 | Loss: 0.0019\n",
      "Epoch: 1137 | Avg Loss: 0.0019\n",
      "max_bleu=0.5915666585834488\n",
      "Epoch: 1138 | Batch: 000 | Loss: 0.0015\n",
      "Epoch: 1138 | Avg Loss: 0.0011\n",
      "max_bleu=0.5915666585834488\n",
      "Epoch: 1139 | Batch: 000 | Loss: 0.0011\n",
      "Epoch: 1139 | Avg Loss: 0.0009\n",
      "max_bleu=0.5915666585834488\n",
      "Epoch: 1140 | Batch: 000 | Loss: 0.0010\n",
      "Epoch: 1140 | Avg Loss: 0.0009\n",
      "目录已存在：../model/xuanmen_km40\n",
      "../model/xuanmen_km40/lstm_kme40_emb256_hid256_frame1_双手合并_多头qkv.pth\n",
      "max_bleu=0.5916014178669367\n",
      "Epoch: 1141 | Batch: 000 | Loss: 0.0003\n",
      "Epoch: 1141 | Avg Loss: 0.0004\n",
      "max_bleu=0.5916014178669367\n",
      "Epoch: 1142 | Batch: 000 | Loss: 0.0002\n",
      "Epoch: 1142 | Avg Loss: 0.0002\n",
      "max_bleu=0.5916014178669367\n",
      "Epoch: 1143 | Batch: 000 | Loss: 0.0002\n",
      "Epoch: 1143 | Avg Loss: 0.0001\n",
      "目录已存在：../model/xuanmen_km40\n",
      "../model/xuanmen_km40/lstm_kme40_emb256_hid256_frame1_双手合并_多头qkv.pth\n",
      "max_bleu=0.5940552703819784\n",
      "Epoch: 1144 | Batch: 000 | Loss: 0.0001\n",
      "Epoch: 1144 | Avg Loss: 0.0001\n",
      "目录已存在：../model/xuanmen_km40\n",
      "../model/xuanmen_km40/lstm_kme40_emb256_hid256_frame1_双手合并_多头qkv.pth\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1145 | Batch: 000 | Loss: 0.0001\n",
      "Epoch: 1145 | Avg Loss: 0.0001\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1146 | Batch: 000 | Loss: 0.0001\n",
      "Epoch: 1146 | Avg Loss: 0.0001\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1147 | Batch: 000 | Loss: 0.0001\n",
      "Epoch: 1147 | Avg Loss: 0.0001\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1148 | Batch: 000 | Loss: 0.0001\n",
      "Epoch: 1148 | Avg Loss: 0.0001\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1149 | Batch: 000 | Loss: 0.0001\n",
      "Epoch: 1149 | Avg Loss: 0.0001\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1150 | Batch: 000 | Loss: 0.0001\n",
      "Epoch: 1150 | Avg Loss: 0.0001\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1151 | Batch: 000 | Loss: 0.0001\n",
      "Epoch: 1151 | Avg Loss: 0.0001\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1152 | Batch: 000 | Loss: 0.0001\n",
      "Epoch: 1152 | Avg Loss: 0.0001\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1153 | Batch: 000 | Loss: 0.0001\n",
      "Epoch: 1153 | Avg Loss: 0.0001\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1154 | Batch: 000 | Loss: 0.0001\n",
      "Epoch: 1154 | Avg Loss: 0.0001\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1155 | Batch: 000 | Loss: 0.0001\n",
      "Epoch: 1155 | Avg Loss: 0.0001\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1156 | Batch: 000 | Loss: 0.0001\n",
      "Epoch: 1156 | Avg Loss: 0.0001\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1157 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1157 | Avg Loss: 0.0001\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1158 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1158 | Avg Loss: 0.0001\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1159 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1159 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1160 | Batch: 000 | Loss: 0.0001\n",
      "Epoch: 1160 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1161 | Batch: 000 | Loss: 0.0001\n",
      "Epoch: 1161 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1162 | Batch: 000 | Loss: 0.0001\n",
      "Epoch: 1162 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1163 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1163 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1164 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1164 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1165 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1165 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1166 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1166 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1167 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1167 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1168 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1168 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1169 | Batch: 000 | Loss: 0.0001\n",
      "Epoch: 1169 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1170 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1170 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1171 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1171 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1172 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1172 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1173 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1173 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1174 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1174 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1175 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1175 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1176 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1176 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1177 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1177 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1178 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1178 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1179 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1179 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1180 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1180 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1181 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1181 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1182 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1182 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1183 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1183 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1184 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1184 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1185 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1185 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1186 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1186 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1187 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1187 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1188 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1188 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1189 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1189 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1190 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1190 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1191 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1191 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1192 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1192 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1193 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1193 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1194 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1194 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1195 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1195 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1196 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1196 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1197 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1197 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1198 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1198 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1199 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1199 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1200 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1200 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1201 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1201 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1202 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1202 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1203 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1203 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1204 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1204 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1205 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1205 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1206 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1206 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1207 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1207 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1208 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1208 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1209 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1209 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1210 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1210 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1211 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1211 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1212 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1212 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1213 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1213 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1214 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1214 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1215 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1215 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1216 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1216 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1217 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1217 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1218 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1218 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1219 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1219 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1220 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1220 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1221 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1221 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1222 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1222 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1223 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1223 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1224 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1224 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1225 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1225 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1226 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1226 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1227 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1227 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1228 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1228 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1229 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1229 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1230 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1230 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1231 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1231 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1232 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1232 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1233 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1233 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1234 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1234 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1235 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1235 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1236 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1236 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1237 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1237 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1238 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1238 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1239 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1239 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1240 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1240 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1241 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1241 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1242 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1242 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1243 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1243 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1244 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1244 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1245 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1245 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1246 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1246 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1247 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1247 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1248 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1248 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1249 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1249 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1250 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1250 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1251 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1251 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1252 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1252 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1253 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1253 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1254 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1254 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1255 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1255 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1256 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1256 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1257 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1257 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1258 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1258 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1259 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1259 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1260 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1260 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1261 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1261 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1262 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1262 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1263 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1263 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1264 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1264 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1265 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1265 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1266 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1266 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1267 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1267 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1268 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1268 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1269 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1269 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1270 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1270 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1271 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1271 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1272 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1272 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1273 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1273 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1274 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1274 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1275 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1275 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1276 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1276 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1277 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1277 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1278 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1278 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1279 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1279 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1280 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1280 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1281 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1281 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1282 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1282 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1283 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1283 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1284 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1284 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1285 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1285 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1286 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1286 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1287 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1287 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1288 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1288 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1289 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1289 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1290 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1290 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1291 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1291 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1292 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1292 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1293 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1293 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1294 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1294 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1295 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1295 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1296 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1296 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1297 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1297 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1298 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1298 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1299 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1299 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1300 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1300 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1301 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1301 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1302 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1302 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1303 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1303 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1304 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1304 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1305 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1305 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1306 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1306 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1307 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1307 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1308 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1308 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1309 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1309 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1310 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1310 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1311 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1311 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1312 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1312 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1313 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1313 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1314 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1314 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1315 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1315 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1316 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1316 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1317 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1317 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1318 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1318 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1319 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1319 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1320 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1320 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1321 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1321 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1322 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1322 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1323 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1323 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1324 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1324 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1325 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1325 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1326 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1326 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1327 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1327 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1328 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1328 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1329 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1329 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1330 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1330 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1331 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1331 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1332 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1332 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1333 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1333 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1334 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1334 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1335 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1335 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1336 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1336 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1337 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1337 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1338 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1338 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1339 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1339 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1340 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1340 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1341 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1341 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1342 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1342 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1343 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1343 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1344 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1344 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1345 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1345 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1346 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1346 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1347 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1347 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1348 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1348 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1349 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1349 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1350 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1350 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1351 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1351 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1352 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1352 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1353 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1353 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1354 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1354 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1355 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1355 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1356 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1356 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1357 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1357 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1358 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1358 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1359 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1359 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1360 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1360 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1361 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1361 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1362 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1362 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1363 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1363 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1364 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1364 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1365 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1365 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1366 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1366 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1367 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1367 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1368 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1368 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1369 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1369 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1370 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1370 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1371 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1371 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1372 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1372 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1373 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1373 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1374 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1374 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1375 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1375 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1376 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1376 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1377 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1377 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1378 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1378 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1379 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1379 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1380 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1380 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1381 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1381 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1382 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1382 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1383 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1383 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1384 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1384 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1385 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1385 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1386 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1386 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1387 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1387 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1388 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1388 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1389 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1389 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1390 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1390 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1391 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1391 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1392 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1392 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1393 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1393 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1394 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1394 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1395 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1395 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1396 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1396 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1397 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1397 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1398 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1398 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1399 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1399 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1400 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1400 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1401 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1401 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1402 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1402 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1403 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1403 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1404 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1404 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1405 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1405 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1406 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1406 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1407 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1407 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1408 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1408 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1409 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1409 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1410 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1410 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1411 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1411 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1412 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1412 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1413 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1413 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1414 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1414 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1415 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1415 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1416 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1416 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1417 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1417 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1418 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1418 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1419 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1419 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1420 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1420 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1421 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1421 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1422 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1422 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1423 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1423 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1424 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1424 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1425 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1425 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1426 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1426 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1427 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1427 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1428 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1428 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1429 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1429 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1430 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1430 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1431 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1431 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1432 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1432 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1433 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1433 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1434 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1434 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1435 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1435 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1436 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1436 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1437 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1437 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1438 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1438 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1439 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1439 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1440 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1440 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1441 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1441 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1442 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1442 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1443 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1443 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1444 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1444 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1445 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1445 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1446 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1446 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1447 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1447 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1448 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1448 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1449 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1449 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1450 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1450 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1451 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1451 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1452 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1452 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1453 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1453 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1454 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1454 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1455 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1455 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1456 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1456 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1457 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1457 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1458 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1458 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1459 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1459 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1460 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1460 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1461 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1461 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1462 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1462 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1463 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1463 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1464 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1464 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1465 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1465 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1466 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1466 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1467 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1467 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1468 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1468 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1469 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1469 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1470 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1470 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1471 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1471 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1472 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1472 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1473 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1473 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1474 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1474 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1475 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1475 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1476 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1476 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1477 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1477 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1478 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1478 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1479 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1479 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1480 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1480 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1481 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1481 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1482 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1482 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1483 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1483 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1484 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1484 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1485 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1485 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1486 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1486 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1487 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1487 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1488 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1488 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1489 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1489 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1490 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1490 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1491 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1491 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1492 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1492 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1493 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1493 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1494 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1494 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1495 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1495 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1496 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1496 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1497 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1497 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1498 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1498 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1499 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1499 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1500 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1500 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1501 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1501 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1502 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1502 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1503 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1503 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1504 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1504 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1505 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1505 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1506 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1506 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1507 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1507 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1508 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1508 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1509 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1509 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1510 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1510 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1511 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1511 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1512 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1512 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1513 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1513 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1514 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1514 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1515 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1515 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1516 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1516 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1517 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1517 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1518 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1518 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1519 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1519 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1520 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1520 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1521 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1521 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1522 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1522 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1523 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1523 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1524 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1524 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1525 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1525 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1526 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1526 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1527 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1527 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1528 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1528 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1529 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1529 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1530 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1530 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1531 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1531 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1532 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1532 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1533 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1533 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1534 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1534 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1535 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1535 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1536 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1536 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1537 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1537 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1538 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1538 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1539 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1539 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1540 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1540 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1541 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1541 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1542 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1542 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1543 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1543 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1544 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1544 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1545 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1545 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1546 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1546 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1547 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1547 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1548 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1548 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1549 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1549 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1550 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1550 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1551 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1551 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1552 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1552 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1553 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1553 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1554 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1554 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1555 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1555 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1556 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1556 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1557 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1557 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1558 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1558 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1559 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1559 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1560 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1560 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1561 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1561 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1562 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1562 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1563 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1563 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1564 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1564 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1565 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1565 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1566 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1566 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1567 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1567 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1568 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1568 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1569 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1569 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1570 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1570 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1571 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1571 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1572 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1572 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1573 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1573 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1574 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1574 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1575 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1575 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1576 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1576 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1577 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1577 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1578 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1578 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1579 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1579 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1580 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1580 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1581 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1581 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1582 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1582 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1583 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1583 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1584 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1584 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1585 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1585 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1586 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1586 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1587 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1587 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1588 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1588 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1589 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1589 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1590 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1590 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1591 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1591 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1592 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1592 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1593 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1593 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1594 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1594 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1595 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1595 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1596 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1596 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1597 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1597 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1598 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1598 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1599 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1599 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1600 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1600 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1601 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1601 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1602 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1602 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1603 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1603 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1604 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1604 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1605 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1605 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1606 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1606 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1607 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1607 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1608 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1608 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1609 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1609 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1610 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1610 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1611 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1611 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1612 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1612 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1613 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1613 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1614 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1614 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1615 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1615 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1616 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1616 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1617 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1617 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1618 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1618 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1619 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1619 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1620 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1620 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1621 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1621 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1622 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1622 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1623 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1623 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1624 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1624 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1625 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1625 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1626 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1626 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1627 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1627 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1628 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1628 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1629 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1629 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1630 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1630 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1631 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1631 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1632 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1632 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1633 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1633 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1634 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1634 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1635 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1635 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1636 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1636 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1637 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1637 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1638 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1638 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1639 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1639 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1640 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1640 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1641 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1641 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1642 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1642 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1643 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1643 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1644 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1644 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1645 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1645 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1646 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1646 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1647 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1647 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1648 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1648 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1649 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1649 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1650 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1650 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1651 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1651 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1652 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1652 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1653 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1653 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1654 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1654 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1655 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1655 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1656 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1656 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1657 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1657 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1658 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1658 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1659 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1659 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1660 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1660 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1661 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1661 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1662 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1662 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1663 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1663 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1664 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1664 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1665 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1665 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1666 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1666 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1667 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1667 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1668 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1668 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1669 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1669 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1670 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1670 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1671 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1671 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1672 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1672 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1673 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1673 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1674 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1674 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1675 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1675 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1676 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1676 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1677 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1677 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1678 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1678 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1679 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1679 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1680 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1680 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1681 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1681 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1682 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1682 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1683 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1683 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1684 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1684 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1685 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1685 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1686 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1686 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1687 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1687 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1688 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1688 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1689 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1689 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1690 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1690 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1691 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1691 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1692 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1692 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1693 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1693 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1694 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1694 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1695 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1695 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1696 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1696 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1697 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1697 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1698 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1698 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1699 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1699 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1700 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1700 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1701 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1701 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1702 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1702 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1703 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1703 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1704 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1704 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1705 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1705 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1706 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1706 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1707 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1707 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1708 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1708 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1709 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1709 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1710 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1710 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1711 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1711 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1712 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1712 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1713 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1713 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1714 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1714 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1715 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1715 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1716 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1716 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1717 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1717 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1718 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1718 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1719 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1719 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1720 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1720 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1721 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1721 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1722 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1722 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1723 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1723 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1724 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1724 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1725 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1725 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1726 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1726 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1727 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1727 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1728 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1728 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1729 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1729 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1730 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1730 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1731 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1731 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1732 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1732 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1733 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1733 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1734 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1734 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1735 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1735 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1736 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1736 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1737 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1737 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1738 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1738 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1739 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1739 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1740 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1740 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1741 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1741 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1742 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1742 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1743 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1743 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1744 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1744 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1745 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1745 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1746 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1746 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1747 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1747 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1748 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1748 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1749 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1749 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1750 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1750 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1751 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1751 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1752 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1752 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1753 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1753 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1754 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1754 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1755 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1755 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1756 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1756 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1757 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1757 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1758 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1758 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1759 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1759 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1760 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1760 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1761 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1761 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1762 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1762 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1763 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1763 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1764 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1764 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1765 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1765 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1766 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1766 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1767 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1767 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1768 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1768 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1769 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1769 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1770 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1770 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1771 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1771 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1772 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1772 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1773 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1773 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1774 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1774 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1775 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1775 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1776 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1776 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1777 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1777 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1778 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1778 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1779 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1779 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1780 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1780 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1781 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1781 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1782 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1782 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1783 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1783 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1784 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1784 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1785 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1785 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1786 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1786 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1787 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1787 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1788 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1788 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1789 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1789 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1790 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1790 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1791 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1791 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1792 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1792 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1793 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1793 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1794 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1794 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1795 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1795 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1796 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1796 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1797 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1797 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1798 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1798 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1799 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1799 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1800 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1800 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1801 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1801 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1802 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1802 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1803 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1803 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1804 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1804 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1805 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1805 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1806 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1806 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1807 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1807 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1808 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1808 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1809 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1809 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1810 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1810 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1811 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1811 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1812 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1812 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1813 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1813 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1814 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1814 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1815 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1815 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1816 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1816 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1817 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1817 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1818 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1818 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1819 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1819 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1820 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1820 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1821 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1821 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1822 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1822 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1823 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1823 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1824 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1824 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1825 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1825 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1826 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1826 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1827 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1827 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1828 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1828 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1829 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1829 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1830 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1830 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1831 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1831 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1832 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1832 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1833 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1833 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1834 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1834 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1835 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1835 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1836 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1836 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1837 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1837 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1838 | Batch: 000 | Loss: 0.0000\n",
      "Epoch: 1838 | Avg Loss: 0.0000\n",
      "max_bleu=0.5959083108745821\n",
      "Epoch: 1839 | Batch: 000 | Loss: 0.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(model)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 你的DataLoader\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0001\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCrossEntropyLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 假设填充符index=0\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# criterion=CrossEntropyLoss(), \u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_bleu\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5885722396528988\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_bool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[23], line 31\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, optimizer, criterion, epochs, device, max_bleu, test_bool)\u001b[0m\n\u001b[1;32m     28\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m     29\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 31\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m03d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Batch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_idx\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m03d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "train(\n",
    "    model=model,\n",
    "    train_loader=train_loader,  # 你的DataLoader\n",
    "    optimizer=optim.Adam(model.parameters(), lr=0.0001),\n",
    "    criterion=CrossEntropyLoss(ignore_index=1),  # 假设填充符index=0\n",
    "    # criterion=CrossEntropyLoss(), \n",
    "    epochs=2000,\n",
    "    device=device,\n",
    "    max_bleu = 0.5885722396528988,\n",
    "    test_bool=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ce1e3e-f32d-4b54-b594-51661c8ab400",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "424：0.595908310"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90c6f4ec-f6f5-402a-8170-4a0800135b4d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目录已存在：../model/xuanmen_km40\n",
      "../model/xuanmen_km40/lstm_kme40_emb256_hid256_frame1_左右手合并.pth\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(savepath):\n",
    "    os.makedirs(savepath)\n",
    "    print(f\"目录已创建：{savepath}\")\n",
    "else:\n",
    "    print(f\"目录已存在：{savepath}\")\n",
    "torch.save(model,os.path.join(savepath,savename))\n",
    "print(os.path.join(savepath,savename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09c7bf75-033c-49d4-92bb-02e0dc6657a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我同学的妈妈是保姆<end><end><end>\n",
      "引导他人成功<end><end><end><end><end><end>\n",
      "他哥哥的同学是医生<end><end><end>\n",
      "我表哥的邻居是记者<end><end><end>\n",
      "他的邻居是残疾人<end><end><end><end>\n",
      "结果圆满成功<end><end><end><end><end><end><end>\n",
      "他哥哥的同学是医生<end><end><end>\n",
      "我的毛毯是新的<end><end><end>\n",
      "他哥哥的目标是解放军<end><end><end>\n",
      "你妹妹是会计<end><end><end><end><end>\n",
      "我们是自由恋爱<end><end><end><end><end><end>\n",
      "紧张的工作气氛<end><end><end><end><end>\n",
      "他的前途事业成功<end><end><end><end><end>\n",
      "妈妈有项链<end><end><end><end><end><end>\n",
      "扭转局面是困难的<end><end><end><end><end>\n",
      "他的工作是美容<end><end><end><end>\n",
      "我有打火机<end><end><end><end><end><end>\n",
      "颜色是丰富的<end><end><end><end><end>\n",
      "我同学的妹妹是护士<end><end><end>\n",
      "紧张的工作气氛<end><end><end><end><end>\n",
      "我同学的妈妈是保姆<end><end><end>\n",
      "天空没有星星<end><end><end><end><end><end>\n",
      "他妈妈的同学是公务员<end><end><end>\n",
      "月亮是地球的卫星<end><end><end><end>\n",
      "社会地位是平等的<end><end><end><end>\n",
      "他们的国家摆脱贫苦<end><end><end><end>\n",
      "他儿子是弱智人<end><end><end><end><end>\n",
      "我公公是门卫<end><end><end><end><end>\n",
      "工作环境的改善<end><end><end><end><end>\n",
      "社会地位的提高<end><end><end><end><end>\n",
      "他儿子是弱智人<end><end><end><end><end>\n",
      "事情有改善<end><end><end><end><end><end>\n",
      "他放弃目标<end><end><end><end><end><end>\n",
      "她外祖父是农民<end><end><end><end><end>\n",
      "社会团结是基础<end><end><end><end><end>\n",
      "紧张的工作气氛<end><end><end><end><end>\n",
      "社会团结是基础<end><end><end><end><end>\n",
      "我的爸爸是商人<end><end><end><end>\n",
      "我同学的妹妹是护士<end><end><end>\n",
      "国民的婚姻幸福<end><end><end><end><end>\n",
      "我们捐献的是毛毯<end><end><end><end>\n",
      "他妹妹的同学是律师<end><end><end>\n",
      "他岳父是残疾人<end><end><end><end><end>\n",
      "你婆婆是盲人<end><end><end><end><end>\n",
      "她外祖父是农民<end><end><end><end><end>\n",
      "我的妻子是教师<end><end><end><end>\n",
      "社会的安定<end><end><end><end><end><end>\n",
      "我的毛巾是干的<end><end><end>\n",
      "我婆婆是保育员<end><end><end><end><end>\n",
      "国家经济情况好<end><end><end><end><end>\n",
      "你婆婆是盲人<end><end><end><end><end>\n",
      "他儿子是弱智人<end><end><end><end><end>\n",
      "他儿子是弱智人<end><end><end><end><end>\n",
      "新的被子是破的<end><end><end>\n",
      "他的外祖母是园丁<end><end><end><end>\n",
      "他哥哥的朋友是演员<end><end><end>\n",
      "民主团结的局势<end><end><end><end><end>\n",
      "裁缝有针线<end><end><end><end><end><end>\n",
      "他妈妈的同学是公务员<end><end><end>\n",
      "我同学的妈妈是保姆<end><end><end>\n",
      "他们的国家摆脱贫苦<end><end><end><end>\n",
      "他的祖父是炊事员<end><end><end><end>\n",
      "他哥哥的朋友是演员<end><end><end>\n",
      "新的被子是破的<end><end><end>\n",
      "他儿子是弱智人<end><end><end><end><end>\n",
      "他妈妈是裁缝<end><end><end><end><end>\n",
      "她外祖父是农民<end><end><end><end><end>\n",
      "他的女朋友是护士<end><end><end><end>\n",
      "我的妻子是教师<end><end><end><end>\n",
      "他招呼你来<end><end><end><end><end>\n",
      "他的牙刷疏<end><end><end><end><end>\n",
      "地球是行星<end><end><end><end><end><end>\n",
      "他弟弟是向导<end><end><end><end><end>\n",
      "我们的国家富强民主<end><end><end><end>\n",
      "结果圆满成功<end><end><end><end><end><end><end>\n",
      "他姐姐的目标是模特<end><end><end>\n",
      "你哥哥是武警<end><end><end><end><end>\n",
      "他的工作是美容<end><end><end><end>\n",
      "他的同学是警察<end><end><end><end>\n",
      "太阳是恒星<end><end><end><end><end><end>\n",
      "我表哥的邻居是记者<end><end><end>\n",
      "他的同学是警察<end><end><end><end>\n",
      "现实是学生任务多<end><end><end><end>\n",
      "我朋友的祖父是工人<end><end><end>\n",
      "我同学的妈妈是保姆<end><end><end>\n",
      "茶壶是褐色的<end><end><end><end><end>\n",
      "他的邻居是残疾人<end><end><end><end>\n",
      "他的同学是警察<end><end><end><end>\n",
      "我同学的妈妈是保姆<end><end><end>\n",
      "我的爸爸是商人<end><end><end><end>\n",
      "他的牙刷疏<end><end><end><end><end>\n",
      "我推荐他去就业<end><end><end><end>\n",
      "他的手表是坏的<end><end><end>\n",
      "我的爸爸是商人<end><end><end><end>\n",
      "颜色是丰富的<end><end><end><end><end>\n",
      "他放弃目标<end><end><end><end><end><end>\n",
      "他哥哥的同学是医生<end><end><end>\n",
      "我们的婚姻是幸福的<end><end><end>\n",
      "你外祖父是猎手<end><end><end><end><end>\n",
      "我的妻子是教师<end><end><end><end>\n",
      "裁缝有针线<end><end><end><end><end><end>\n",
      "杯子是橙色的<end><end><end><end><end>\n",
      "你婆婆是盲人<end><end><end><end><end>\n",
      "他的祖父是炊事员<end><end><end><end>\n",
      "月亮是地球的卫星<end><end><end><end>\n",
      "你外祖父是猎手<end><end><end><end><end>\n",
      "你哥哥是武警<end><end><end><end><end>\n",
      "他的女朋友是护士<end><end><end><end>\n",
      "工作效益稳定提高<end><end><end><end><end>\n",
      "他的牙刷疏<end><end><end><end><end>\n",
      "国家经济情况好<end><end><end><end><end>\n",
      "扭转局面是困难的<end><end><end><end><end>\n",
      "我嫂嫂是画家<end><end><end><end><end>\n",
      "他的祖父是炊事员<end><end><end><end>\n",
      "他外祖父是邮递员<end><end><end><end><end>\n",
      "茶壶是褐色的<end><end><end><end><end>\n"
     ]
    }
   ],
   "source": [
    "for src, trg in test_loader:\n",
    "    src = src.to(device)\n",
    "    output = model.predict(src)\n",
    "    output = output[0].tolist()\n",
    "    text = ''\n",
    "    for i in output:\n",
    "        text+=idx2word[i]\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "15e3932a-4bc5-4733-a958-b773f96f382e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class seq2seq(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hidden_dim,output_dim,n_layers):\n",
    "        super().__init__()\n",
    "        self.encode_embedding = nn.Embedding(input_dim, emb_dim) #将每个词扩充为emb_dim维\n",
    "        self.decode_embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.encode = nn.LSTM(emb_dim, hidden_dim, n_layers)\n",
    "        self.decode = nn.LSTM(emb_dim,hidden_dim, n_layers)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "#     def forward(self, src,tar):\n",
    "#         # src: [batch_size, src_len]\n",
    "#         # trg: [batch_size, trg_len]\n",
    "#         encode_embedded = self.encode_embedding(src)  # [batch_size, src_len, emb_dim]\n",
    "#         encode_embedded = encode_embedded.permute(1, 0, 2) \n",
    "#         print(f'embedded:{encode_embedded.size()}')\n",
    "#         outputs, (hidden, cell) = self.encode(encode_embedded)\n",
    "#         print(f'encode outputs:{outputs.size()}')\n",
    "#         print(f'encode hidden:{hidden.size()}')\n",
    "#         decode_embedded = self.decode_embedding(tar) \n",
    "#         print(f'decode embedden:{decode_embedded.size()}')\n",
    "#         # outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
    "        \n",
    "#         # outputs,_ = self.decode(hidden)\n",
    "#         return hidden\n",
    "    def forward(self, src, tar):\n",
    "        # src: [batch_size, src_len]\n",
    "        # tar: [batch_size, trg_len]\n",
    "        \n",
    "        # 编码器部分\n",
    "        encode_embedded = self.encode_embedding(src)  # [batch_size, src_len, emb_dim]\n",
    "        encode_embedded = encode_embedded.permute(1, 0, 2)  # [src_len, batch_size, emb_dim]\n",
    "        _, (hidden, cell) = self.encode(encode_embedded)\n",
    "        \n",
    "        # 解码器部分\n",
    "        batch_size = tar.shape[0] #3\n",
    "        trg_len = tar.shape[1] #9\n",
    "        output_dim = self.fc.out_features #181\n",
    "        print(output_dim)\n",
    "        \n",
    "        # 准备输出张量\n",
    "        outputs = torch.zeros(trg_len, batch_size, output_dim).to(src.device)#9x3x181\n",
    "        \n",
    "        # 初始输入是<sos> token，这里假设tar已经包含<sos>作为第一个token\n",
    "        input = tar[:, 0]  # 取第一个token作为初始输入 [batch_size]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            # 嵌入输入\n",
    "            embedded = self.decode_embedding(input).unsqueeze(0)  # [1, batch_size, emb_dim]\n",
    "            print(f'embedded:{embedded.size()}')\n",
    "            print(f'hidden:{hidden.size()}')\n",
    "            # 通过解码器\n",
    "            output, (hidden, cell) = self.decode(embedded, (hidden, cell))\n",
    "            \n",
    "            # 预测下一个token\n",
    "            pred = self.fc(output.squeeze(0))\n",
    "            outputs[t] = pred\n",
    "            \n",
    "            # 下一个输入是真实目标(teacher forcing)或预测结果\n",
    "            # 这里使用teacher forcing，传入真实目标\n",
    "            input = tar[:, t]\n",
    "        \n",
    "        return outputs.permute(1, 0, 2)  # [batch_size, trg_len, output_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4aa861a0-d00f-4846-a38a-c200065c44ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 9])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tar.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f4d04e70-4d5a-452a-a078-77231a1e800e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([47,  6, 74])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tar[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4dff98-5436-4675-aeee-1f2a551aac24",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.data =all_data \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx][1]\n",
    "        label = self.data[idx][0]\n",
    "        return sample, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "10039ced-ba68-46b8-b85f-94d093d0b34a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 定义划分比例（例如80%训练，20%测试）\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "# 随机划分\n",
    "train_dataset, test_dataset = random_split(\n",
    "    dataset, \n",
    "    [train_size, test_size],\n",
    "    generator=torch.Generator().manual_seed(42)  # 固定随机种子确保可复现\n",
    ")\n",
    "\n",
    "# 创建DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=3, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e0a0af9-5dc9-4048-88d7-57996ab05072",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([18, 18, 26, 18, 26, 18, 26, 18, 26, 18, 26, 18, 26, 18, 26, 18, 26, 18,\n",
       "         26, 18, 26, 18, 26, 18, 26, 18, 26, 18, 26, 18, 26, 18, 26, 18, 26, 18,\n",
       "         26, 18, 26, 18, 26, 18, 26, 18, 26, 18, 26, 18, 26, 18, 26, 18, 26, 18,\n",
       "         26, 26, 18, 18, 18, 18, 24, 18, 24, 24, 18, 18, 18, 18, 18, 18, 28, 18,\n",
       "         35, 18, 38, 18, 38, 18, 38, 18, 38, 38, 38, 18, 38, 38, 38, 18, 38, 38,\n",
       "         38, 18, 38, 38, 38, 18, 38, 38, 38, 18, 38, 38, 38, 18, 38, 38, 38, 18,\n",
       "         38, 18, 38, 18, 38, 18, 35, 18, 31, 35, 31, 35, 28, 35, 28, 35, 35, 19,\n",
       "          4, 18,  4, 18, 28,  4, 35,  4, 35, 18,  6, 18, 38,  5,  1, 18,  1, 18,\n",
       "          1, 18,  1,  1,  1, 18,  1, 18,  1, 18,  1,  1,  1, 18,  1,  1, 34, 18,\n",
       "          6, 34, 34, 18, 38, 34, 34, 18,  1, 18, 16, 18, 18, 18, 18, 18, 18, 18,\n",
       "         18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
       "         18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 32, 18, 27, 18, 12, 18, 12, 18,\n",
       "         12, 18, 12,  1,  1, 18,  1,  1,  1, 12,  1, 12, 13, 12, 13, 12, 13, 12,\n",
       "         13, 12, 13, 12, 13, 12, 13, 12, 13, 12,  1, 12,  1, 12,  1, 12,  1, 12,\n",
       "          1, 12,  1, 12,  1, 12,  1, 12,  1, 12,  1, 12,  1,  1,  1,  1, 20, 18,\n",
       "         20, 18, 20, 36, 17, 18, 16, 18, 16, 18, 27, 18, 31, 18, 13, 18, 13, 18,\n",
       "         13, 18, 31, 18, 38, 18, 38, 18, 38, 18, 35, 18, 34, 18, 34, 18, 34, 34,\n",
       "         34, 18, 34, 18, 34, 18, 34, 18, 34, 18, 34, 18, 34, 34, 34, 18, 34, 34,\n",
       "         34, 18, 34, 20, 20, 18, 34, 20, 20, 18, 34, 20, 20, 18, 16,  7,  7, 18,\n",
       "          7,  7, 18, 18, 18, 18, 18, 18,  7, 18,  7, 18,  7, 18,  7, 18,  7, 18,\n",
       "          7, 18,  7, 18,  7, 18,  7, 18,  7, 18,  7, 18, 20, 18, 20, 18, 20, 18,\n",
       "         20, 18, 20, 18, 20, 18, 20, 18,  7, 20, 20, 18, 16, 20, 18, 18, 18, 18,\n",
       "         18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
       "         18, 18, 18, 18, 18, 18, 18, 18, 18, 18], dtype=torch.int32),\n",
       " tensor([ 5,  3,  9,  4, 49,  2,  0,  0]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdf01de-85b9-48ed-92da-d4473b22c4a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "e7fdcfc6-ace0-4514-93d0-19a4fb11b8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# class Seq2SeqTransformer(nn.Module):\n",
    "#     def __init__(self, input_dim, output_dim, d_model=512, nhead=8, num_layers=6):\n",
    "#         super().__init__()\n",
    "#         self.encoder = nn.Linear(input_dim, d_model)\n",
    "#         self.decoder = nn.Linear(d_model, output_dim)\n",
    "#         self.transformer = nn.Transformer(\n",
    "#             d_model=d_model,\n",
    "#             nhead=nhead,\n",
    "#             num_encoder_layers=num_layers,\n",
    "#             num_decoder_layers=num_layers\n",
    "#         )\n",
    "#         self.pos_encoder = PositionalEncoding(d_model)  # 需自定义\n",
    "\n",
    "#     def forward(self, src, tgt):\n",
    "#         # src: (seq_len, batch, input_dim)\n",
    "#         src = self.encoder(src)  # (seq_len, batch, d_model)\n",
    "#         src = self.pos_encoder(src)\n",
    "#         tgt = self.pos_encoder(tgt)  # 假设tgt是decoder输入\n",
    "#         output = self.transformer(src, tgt)\n",
    "#         return self.decoder(output)\n",
    "\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, output_size, d_model=128, nhead=4, num_layers=3):\n",
    "        super().__init__()\n",
    "        self.src_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.tgt_embedding = nn.Embedding(output_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers\n",
    "        )\n",
    "        self.decoder = nn.Linear(d_model, output_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        # 确保输入至少有3维\n",
    "        if src.dim() == 2:\n",
    "            src = src.unsqueeze(1)  # (seq_len, 1, input_dim)\n",
    "        if tgt.dim() == 2:\n",
    "            tgt = tgt.unsqueeze(1)  # (seq_len, 1, output_dim)\n",
    "\n",
    "        src = self.pos_encoder(self.src_embedding(src))\n",
    "        tgt = self.pos_encoder(self.tgt_embedding(tgt))\n",
    "        output = self.transformer(src, tgt)\n",
    "        return self.decoder(output)\n",
    "\n",
    "# 位置编码（Transformer需要）\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        pe = self.pe[:x.size(0)].unsqueeze(1)\n",
    "        x = x + pe\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.encoder = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.decoder = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        embedded = embedded.permute(1, 0, 2) \n",
    "        _, (hidden, cell) = self.encoder(embedded)\n",
    "        output, _ = self.decoder(embedded, (hidden, cell))\n",
    "        output = self.fc(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "15d55c09-a589-447c-a2ba-3228179565ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.encoder = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.decoder = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        embedded = embedded.permute(1, 0, 2) \n",
    "        _, (hidden, cell) = self.encoder(embedded)\n",
    "        output, _ = self.decoder(embedded, (hidden, cell))\n",
    "        # output = self.fc(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c5deb7-3956-4c7d-9218-561bde25c963",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, max_output_len=8):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.encoder = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.decoder = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.max_output_len = max_output_len  # 固定输出长度8\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        embedded = self.embedding(x)  # [batch, 424, hidden_size]\n",
    "        _, (hidden, cell) = self.encoder(embedded)  # hidden: [1, batch, hidden_size]\n",
    "\n",
    "        # Decoder初始化\n",
    "        batch_size = x.size(0)\n",
    "        decoder_input = torch.zeros(batch_size, 1, dtype=torch.long).to(x.device)  # 初始输入<SOS>（假设0是<SOS>）\n",
    "        outputs = torch.zeros(batch_size, self.max_output_len, self.output_size).to(x.device)\n",
    "\n",
    "        # 自回归生成（逐步预测）\n",
    "        for t in range(self.max_output_len):\n",
    "            decoder_embedded = self.embedding(decoder_input)  # [batch, 1, hidden_size]\n",
    "            decoder_output, (hidden, cell) = self.decoder(decoder_embedded, (hidden, cell))\n",
    "            output = self.fc(decoder_output.squeeze(1))  # [batch, output_size]\n",
    "            outputs[:, t, :] = output\n",
    "\n",
    "            # 下一步输入是当前预测的token（Teacher Forcing可选）\n",
    "            decoder_input = output.argmax(-1).unsqueeze(1)  # [batch, 1]\n",
    "\n",
    "        return outputs  # [batch, 8, output_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "1314d6d0-1b07-4c27-9b39-9bf2a4bcfe4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# vocab_size = 40  # 词汇表大小（根据你的token ID最大值38，建议取稍大的值如50）\n",
    "# output_size = 180  # 输出维度（与词汇表大小一致，如果是分类任务）\n",
    "\n",
    "# model = Seq2SeqTransformer(vocab_size=vocab_size, output_size=output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "dc700a1d-cdf7-4fe3-8bc9-fb74c49a802c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_size = 40\n",
    "hidden_size = 64\n",
    "output_size = 180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "ffb5a17a-1ba8-46ed-b9e5-41ed811529ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Seq2Seq(input_size, hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "e36efc20-7f73-4ca7-aa64-d0db51a3c08c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 2 is not equal to len(dims) = 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[271], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (8, 1, 50)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(output\u001b[38;5;241m.\u001b[39msize())\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[268], line 11\u001b[0m, in \u001b[0;36mSeq2Seq.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     10\u001b[0m     embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(x)\n\u001b[0;32m---> 11\u001b[0m     embedded \u001b[38;5;241m=\u001b[39m \u001b[43membedded\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[1;32m     12\u001b[0m     _, (hidden, cell) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(embedded)\n\u001b[1;32m     13\u001b[0m     output, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(embedded, (hidden, cell))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 2 is not equal to len(dims) = 3"
     ]
    }
   ],
   "source": [
    "output = model(src)  # (8, 1, 50)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "7ea871bc-8736-4014-81b4-ef7c1a59fb1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 424])\n",
      "torch.Size([2, 8])\n",
      "torch.Size([2, 424, 180])\n"
     ]
    }
   ],
   "source": [
    "model = Seq2Seq(input_size, hidden_size, output_size)\n",
    "for input_tensor, target_tensor in train_loader:\n",
    "        print(input_tensor.size())\n",
    "        print(target_tensor.size())\n",
    "        output = model(input_tensor)\n",
    "        print(output.size())\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f182d0-104d-4371-8fbc-82f2a4e6d368",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c9468d9b-bbff-4dd8-9c12-861c038640bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "src = torch.tensor([18, 18, 26, 18, 26, 18, 26, 18, 26, 18, 26, 18, 26, 18, 26, 18, 26, 18,\n",
    "         26, 18, 26, 18, 26, 18, 26, 18, 26, 18, 26, 18, 26, 18, 26, 18, 26, 18,\n",
    "         26, 18, 26, 18, 26, 18, 26, 18, 26, 18, 26, 18, 26, 18, 26, 18, 26, 18,\n",
    "         26, 26, 18, 18, 18, 18, 24, 18, 24, 24, 18, 18, 18, 18, 18, 18, 28, 18,\n",
    "         35, 18, 38, 18, 38, 18, 38, 18, 38, 38, 38, 18, 38, 38, 38, 18, 38, 38,\n",
    "         38, 18, 38, 38, 38, 18, 38, 38, 38, 18, 38, 38, 38, 18, 38, 38, 38, 18,\n",
    "         38, 18, 38, 18, 38, 18, 35, 18, 31, 35, 31, 35, 28, 35, 28, 35, 35, 19,\n",
    "          4, 18,  4, 18, 28,  4, 35,  4, 35, 18,  6, 18, 38,  5,  1, 18,  1, 18,\n",
    "          1, 18,  1,  1,  1, 18,  1, 18,  1, 18,  1,  1,  1, 18,  1,  1, 34, 18,\n",
    "          6, 34, 34, 18, 38, 34, 34, 18,  1, 18, 16, 18, 18, 18, 18, 18, 18, 18,\n",
    "         18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
    "         18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 32, 18, 27, 18, 12, 18, 12, 18,\n",
    "         12, 18, 12,  1,  1, 18,  1,  1,  1, 12,  1, 12, 13, 12, 13, 12, 13, 12,\n",
    "         13, 12, 13, 12, 13, 12, 13, 12, 13, 12,  1, 12,  1, 12,  1, 12,  1, 12,\n",
    "          1, 12,  1, 12,  1, 12,  1, 12,  1, 12,  1, 12,  1,  1,  1,  1, 20, 18,\n",
    "         20, 18, 20, 36, 17, 18, 16, 18, 16, 18, 27, 18, 31, 18, 13, 18, 13, 18,\n",
    "         13, 18, 31, 18, 38, 18, 38, 18, 38, 18, 35, 18, 34, 18, 34, 18, 34, 34,\n",
    "         34, 18, 34, 18, 34, 18, 34, 18, 34, 18, 34, 18, 34, 34, 34, 18, 34, 34,\n",
    "         34, 18, 34, 20, 20, 18, 34, 20, 20, 18, 34, 20, 20, 18, 16,  7,  7, 18,\n",
    "          7,  7, 18, 18, 18, 18, 18, 18,  7, 18,  7, 18,  7, 18,  7, 18,  7, 18,\n",
    "          7, 18,  7, 18,  7, 18,  7, 18,  7, 18,  7, 18, 20, 18, 20, 18, 20, 18,\n",
    "         20, 18, 20, 18, 20, 18, 20, 18,  7, 20, 20, 18, 16, 20, 18, 18, 18, 18,\n",
    "         18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
    "         18, 18, 18, 18, 18, 18, 18, 18, 18, 18])\n",
    "tgt = torch.tensor([ 5,  3,  9,  4, 49,  2,  0,  0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ccd9376f-5db8-4f54-bcb1-161ff9bd7f71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "src = torch.tensor([list(src),list(src)])\n",
    "tgt = torch.tensor([list(tgt),list(tgt)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "08596ac8-eb3e-407f-8911-a310cee29e4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output = model(src)  # (8, 1, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "eadfb7d9-bfb5-403b-9ee2-b12c73ffdcbe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 424, 180])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "de711765-aa68-4c75-913f-89e0fdc9d67e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4f730f07-73b7-43c4-85c5-22a692d1a652",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 180])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bff2ae2b-8b52-4c56-8927-7c04735f225f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)  # 忽略padding值（假设0是padding）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7540d0c6-0727-4474-83a7-a2b4efa16b68",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/464 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "424\n",
      "tensor([[  5,  87,   3,  26,   4, 129,   2,   0],\n",
      "        [  5,  20,   3,   9,   4, 106,   2,   0],\n",
      "        [105,  28,   3,  19,   2,   0,   0,   0],\n",
      "        [  7,  27,   4, 151,   2,   0,   0,   0],\n",
      "        [  8, 115,  15,   2,   0,   0,   0,   0],\n",
      "        [ 23,   3, 168,   4, 152,   3,   2,   0],\n",
      "        [ 13, 121,  28,  22,   2,   0,   0,   0],\n",
      "        [ 48,  10, 175,   2,   0,   0,   0,   0],\n",
      "        [  5, 154,   4, 162,   2,   0,   0,   0],\n",
      "        [112,   4,  32,   3,   2,   0,   0,   0],\n",
      "        [ 55,   3,  12, 119, 173,   2,   0,   0],\n",
      "        [  7,  27,   4, 151,   2,   0,   0,   0],\n",
      "        [ 68,  12,   3,  59,   2,   0,   0,   0],\n",
      "        [ 13, 121,  28,  22,   2,   0,   0,   0],\n",
      "        [  5,   3,  80,   4,  76,   2,   0,   0],\n",
      "        [  6,   3,  23,  95,   4, 132,   3,   2],\n",
      "        [  6,   3,  17,   4,  93,   2,   0,   0],\n",
      "        [ 67,   4, 176,   3,   2,   0,   0,   0],\n",
      "        [  8,   3,  38,   4,  21,   3,   2,   0],\n",
      "        [  5,  98,   4,  44,   2,   0,   0,   0],\n",
      "        [  6, 167,   3,  50,   4, 172,   2,   0],\n",
      "        [  7,  14,   4, 131,   2,   0,   0,   0],\n",
      "        [  6, 117,   5,  71,  39,   2,   0,   0],\n",
      "        [  5,  15,   4, 174,   2,   0,   0,   0],\n",
      "        [ 48,  10, 175,   2,   0,   0,   0,   0],\n",
      "        [  8, 116,   3,   4,  45,   2,   0,   0],\n",
      "        [ 77,   3,  38,  21,   2,   0,   0,   0],\n",
      "        [  5,   3, 149,   4, 159,   3,   2,   0],\n",
      "        [ 73,   8,   3,  57,   2,   0,   0,   0],\n",
      "        [179,   4,  51,   3,   2,   0,   0,   0],\n",
      "        [  6,  29,   4, 163,   2,   0,   0,   0],\n",
      "        [170,   5,   3,  41,   2,   0,   0,   0]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (424) must match the size of tensor b (32) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m tgt_output \u001b[38;5;241m=\u001b[39m tgt[\u001b[38;5;241m1\u001b[39m:, :]   \u001b[38;5;66;03m# 去掉第一个token\u001b[39;00m\n\u001b[1;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 17\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_input\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (seq_len, batch, vocab_size)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# 计算损失（忽略padding）\u001b[39;00m\n\u001b[1;32m     20\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, output\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)), \n\u001b[1;32m     21\u001b[0m                tgt_output\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[16], line 39\u001b[0m, in \u001b[0;36mSeq2SeqTransformer.forward\u001b[0;34m(self, src, tgt)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, src, tgt):\n\u001b[0;32m---> 39\u001b[0m     src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msrc_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (src_len, batch, d_model)\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     tgt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_encoder(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtgt_embedding(tgt))  \u001b[38;5;66;03m# (tgt_len, batch, d_model)\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(src, tgt)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[16], line 56\u001b[0m, in \u001b[0;36mPositionalEncoding.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 56\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpe\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (424) must match the size of tensor b (32) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "n_epochs = 40\n",
    "for epoch in range(n_epochs):\n",
    "    # 训练阶段\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for src, tgt in tqdm(train_loader, desc=f'Epoch {epoch+1}'):\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        print(len(src))\n",
    "        print(len(src[0]))\n",
    "        print(tgt)\n",
    "\n",
    "        # 准备decoder输入（shifted right）\n",
    "        tgt_input = tgt[:-1, :]  # 去掉最后一个token\n",
    "        tgt_output = tgt[1:, :]   # 去掉第一个token\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt_input)  # (seq_len, batch, vocab_size)\n",
    "\n",
    "        # 计算损失（忽略padding）\n",
    "        loss = criterion(output.view(-1, output.size(-1)), \n",
    "                       tgt_output.reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # 验证阶段\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in val_loader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            tgt_input = tgt[:-1, :]\n",
    "            tgt_output = tgt[1:, :]\n",
    "\n",
    "            output = model(src, tgt_input)\n",
    "            loss = criterion(output.view(-1, output.size(-1)), \n",
    "                           tgt_output.reshape(-1))\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    # 打印日志\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f'Epoch {epoch+1}: Train Loss={avg_train_loss:.4f}, Val Loss={avg_val_loss:.4f}')\n",
    "\n",
    "    # 保存最佳模型\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877324c7-8a44-47ad-b01b-7bcf4f1e9823",
   "metadata": {},
   "source": [
    "#### import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm  # 进度条工具\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)  # 忽略padding值（假设0是padding）\n",
    "    \n",
    "    # 开始训练\n",
    "    trained_model = train_model(\n",
    "        model, train_data, val_data, \n",
    "        optimizer, criterion,\n",
    "        n_epochs=10, batch_size=2, device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb7e638-ed33-4215-b831-60e2e5a927fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabb1655-4031-4cbf-b823-5355bb805af1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05d6b35-c2f5-4754-9395-c93e6ba13cdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "5e32c2a3-4ed8-4450-8d9d-c3466baa46e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        _, (hidden, cell) = self.lstm(x)\n",
    "        return hidden, cell\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, hidden_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(output_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x, hidden, cell):\n",
    "        output, (hidden, cell) = self.lstm(x, (hidden, cell))\n",
    "        prediction = self.fc(output)\n",
    "        return prediction, hidden, cell\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, source, target, teacher_forcing_ratio=0.5):\n",
    "        batch_size = target.shape[0]\n",
    "        target_len = target.shape[1]\n",
    "        target_dim = target.shape[2]\n",
    "        \n",
    "        # 存储输出\n",
    "        outputs = torch.zeros(batch_size, target_len, target_dim).to(self.device)\n",
    "        \n",
    "        # 编码器处理\n",
    "        hidden, cell = self.encoder(source)\n",
    "        \n",
    "        # 第一个输入是起始token (全零)\n",
    "        input = torch.zeros(batch_size, 1, target_dim).to(self.device)\n",
    "        \n",
    "        for t in range(target_len):\n",
    "            # 解码器一步\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            \n",
    "            # 存储预测\n",
    "            outputs[:, t:t+1] = output\n",
    "            \n",
    "            # 决定是否使用teacher forcing\n",
    "            teacher_force = np.random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            # 如果使用teacher forcing，下一个输入是真实值；否则使用预测值\n",
    "            input = target[:, t:t+1] if teacher_force else output\n",
    "            \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "9011533a-c299-4183-a013-647706b8a428",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[133], line 64\u001b[0m\n\u001b[1;32m     61\u001b[0m best_valid_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m---> 64\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m     valid_loss \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader, criterion)\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;66;03m# 保存最佳模型\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[133], line 26\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, criterion, clip)\u001b[0m\n\u001b[1;32m     22\u001b[0m src, trg \u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39mto(device), trg\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     24\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 26\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, trg)\n\u001b[1;32m     30\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[132], line 31\u001b[0m, in \u001b[0;36mSeq2Seq.forward\u001b[0;34m(self, source, target, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m     29\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     30\u001b[0m target_len \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 31\u001b[0m target_dim \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# 存储输出\u001b[39;00m\n\u001b[1;32m     34\u001b[0m outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(batch_size, target_len, target_dim)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "# 参数设置\n",
    "input_dim = 424  # 输入特征维度\n",
    "output_dim = 8  # 输出特征维度\n",
    "hidden_dim = 256\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 初始化模型\n",
    "encoder = Encoder(input_dim, hidden_dim)\n",
    "decoder = Decoder(output_dim, hidden_dim)\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "\n",
    "# 定义优化器和损失函数\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = nn.MSELoss()  # 对于回归任务\n",
    "\n",
    "# 训练函数\n",
    "def train(model, dataloader, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, (src, trg) in enumerate(train_loader):\n",
    "        src, trg = src.to(device), trg.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, trg)\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # 梯度裁剪防止爆炸\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "# 验证函数\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (src, trg) in enumerate(test_loader):\n",
    "            src, trg = src.to(device), trg.to(device)\n",
    "            \n",
    "            output = model(src, trg, 0)  # 关闭teacher forcing\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "# 训练循环\n",
    "n_epochs = 100\n",
    "clip = 1\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train(model, train_loader, optimizer, criterion, clip)\n",
    "    valid_loss = evaluate(model, val_loader, criterion)\n",
    "    \n",
    "    # 保存最佳模型\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
    "    print(f'\\tVal. Loss: {valid_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466787bf-1455-4612-ba79-ab0542d5a6f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
